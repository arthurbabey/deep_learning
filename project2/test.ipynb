{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class Linear():\n",
    "\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "\n",
    "        # constant used for initializing weights\n",
    "        eps = 1e-2\n",
    "\n",
    "        # weights for the layer\n",
    "        self.w = torch.empty(dim_out, dim_in).normal_(0, eps)\n",
    "\n",
    "        # bias for the layer\n",
    "        self.b = torch.empty(dim_out).normal_(0, eps)\n",
    "\n",
    "        # Corresponding gradients\n",
    "\n",
    "        self.dw = torch.zeros_like(self.w)\n",
    "        self.db = torch.zeros_like(self.b)\n",
    "\n",
    "        # Temporary variable to store module's previous input for use in backward pass\n",
    "        self.temp = torch.zeros(dim_in)\n",
    "\n",
    "\n",
    "    def forward(self, x_in):\n",
    "\n",
    "        if x_in.dim() > 1:\n",
    "            x_out =  torch.mm(self.w, x_in) + self.b         # To handle input of vector form\n",
    "        else:\n",
    "            x_out =  torch.mv(self.w,x_in) + self.b         # To handle input of tensor form\n",
    "        self.temp = x_in\n",
    "        return x_out\n",
    "\n",
    "    def gradient(self):         #gradient of output vs input\n",
    "\n",
    "        return self.w.t()\n",
    "\n",
    "    def backward(self, gradwrtoutput, x_in = None):\n",
    "        \"\"\"\n",
    "        :param gradwrtoutput: Gradient of loss wrt module's output\n",
    "        :param x_in: Module's input\n",
    "        :return: Gradient of loss wrt module's input\n",
    "        \"\"\"\n",
    "\n",
    "        if x_in is None:            # If x_in is not provided, it's taken from the forward pass\n",
    "            x_in = self.temp\n",
    "\n",
    "        self.dw += torch.ger(gradwrtoutput, x_in.t())       # Accumulate gradient wrt parameters\n",
    "        self.db += gradwrtoutput\n",
    "\n",
    "        dldx_in = torch.mv(self.w.t(), gradwrtoutput)   # Gradient of loss wrt the module's input\n",
    "        return dldx_in\n",
    "\n",
    "\n",
    "    def param(self):\n",
    "        paramlist = [self.w] + [self.b]\n",
    "        gradlist = [self.dw] + [self.db]\n",
    "        return paramlist, gradlist\n",
    "\n",
    "    def grad_zero(self):\n",
    "        self.dw = torch.zeros_like(self.w)\n",
    "        self.db = torch.zeros_like(self.b)\n",
    "\n",
    "    __call__ = forward\n",
    "\n",
    "class relu():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.temp = []        # Temporary variable to store module's previous input for use in backward pass\n",
    "\n",
    "\n",
    "    def forward(self, x_in):\n",
    "\n",
    "        self.temp = x_in\n",
    "        return torch.where(x_in> 0, x_in, torch.zeros_like(x_in))\n",
    "\n",
    "    def gradient(self, x_in):\n",
    "\n",
    "        return torch.where(x_in> 0, torch.ones_like(x_in), torch.zeros_like(x_in))\n",
    "\n",
    "    def backward(self, gradwrtoutput, x_in = None):\n",
    "        \"\"\"\n",
    "        :param gradwrtoutput: Gradient of loss wrt module's output\n",
    "        :param x_in: Module's input\n",
    "        :return: Gradient of loss wrt module's input\n",
    "        \"\"\"\n",
    "\n",
    "        if x_in == None:            # If x_in is not provided, it's taken from the forward pass\n",
    "            x_in = self.temp\n",
    "\n",
    "        dldx_in = torch.mul(gradwrtoutput, self.gradient(x_in))     #Compute gradient wrt input of module\n",
    "        return dldx_in\n",
    "\n",
    "    def param(self):\n",
    "\n",
    "        return []\n",
    "\n",
    "    def grad_zero(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    __call__ = forward\n",
    "    \n",
    "class Tanh():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.temp = []        # Temporary variable to store module's previous input for use in backward pass\n",
    "\n",
    "\n",
    "    def forward(self, x_in):\n",
    "\n",
    "        self.temp = x_in\n",
    "        return (torch.exp(x_in) - torch.exp(-x_in)) / (torch.exp(x_in) + torch.exp(-x_in))\n",
    "\n",
    "    def gradient(self, x_in):\n",
    "\n",
    "        return 1 - torch.pow(self.forward(x_in), 2)\n",
    "\n",
    "    def backward(self, gradwrtoutput, x_in = None):\n",
    "        \"\"\"\n",
    "        :param gradwrtoutput: Gradient of loss wrt module's output\n",
    "        :param x_in: Module's input\n",
    "        :return: Gradient of loss wrt module's input\n",
    "        \"\"\"\n",
    "\n",
    "        if x_in == None:            # If x_in is not provided, it's taken from the forward pass\n",
    "            x_in = self.temp\n",
    "\n",
    "        dldx_in = torch.mul(gradwrtoutput, self.gradient(x_in))     #Compute gradient wrt input of module\n",
    "        return dldx_in\n",
    "\n",
    "    def param(self):\n",
    "\n",
    "        return []\n",
    "\n",
    "    def grad_zero(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    __call__ = forward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class loss_MSE():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x_out, x_target):\n",
    "\n",
    "        return torch.sum(torch.pow(x_out-x_target,2))\n",
    "\n",
    "    def backward(self, x_out, x_target):\n",
    "\n",
    "        return 2*(x_out - x_target)\n",
    "\n",
    "    def param(self):\n",
    "\n",
    "        return []\n",
    "\n",
    "    __call__ = forward\n",
    "\n",
    "\n",
    "class Sequential():\n",
    "\n",
    "    def __init__(self, *modules):\n",
    "        self.layers = modules\n",
    "        self.temp = []        # Temporary variable to store module's previous input for use in backward pass\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        \"\"\"\n",
    "        function for forward pass through all layers in the sequential list\n",
    "        :param x_in: input data\n",
    "        :return: output processed data\n",
    "        \"\"\"\n",
    "        x_out = torch.zeros_like(x_in)\n",
    "\n",
    "        for layer in self.layers:           #Call forward function of each layer\n",
    "            x_out = layer.forward(x_in)\n",
    "            x_in = x_out\n",
    "        self.temp = x_in\n",
    "        return x_out\n",
    "\n",
    "    __call__ = forward\n",
    "\n",
    "    def backward(self, gradwrtoutput, x_in = None):\n",
    "        \"\"\"\n",
    "        :param gradwrtoutput: Gradient of loss wrt module's output\n",
    "        :param x_in: Module's input\n",
    "        :return: Gradient of loss wrt module's input\n",
    "        \"\"\"\n",
    "\n",
    "        if x_in is None:            # If x_in is not provided, it's taken from the forward pass\n",
    "            x_in = self.temp\n",
    "        grad_in = gradwrtoutput\n",
    "        count = len(self.layers)\n",
    "        for i in range(0,count):\n",
    "            grad_out = self.layers[count-i-1].backward(grad_in)\n",
    "            grad_in = grad_out\n",
    "\n",
    "    def param(self):\n",
    "\n",
    "        paramlist = []\n",
    "        gradlist = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            try:\n",
    "                layer_param, layer_grad = layer.param()\n",
    "                paramlist = paramlist + layer_param\n",
    "                gradlist = gradlist + layer_grad\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return paramlist, gradlist\n",
    "\n",
    "    def grad_zero(self):\n",
    "        for layer in self.layers:\n",
    "            layer.grad_zero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_sample):\n",
    "    \n",
    "    dist = 1/math.sqrt(2*math.pi)\n",
    "    \n",
    "    train_input = torch.empty(n_sample, 2).uniform_(0, 1)\n",
    "    test_input = torch.empty(n_sample, 2).uniform_(0, 1)\n",
    "    \n",
    "    train_target = (train_input - 0.5).norm(p=2, dim=1)\n",
    "    train_target[train_target > dist] = 1\n",
    "    train_target[train_target <= dist] = 0\n",
    "    train_target = 1 - train_target\n",
    "    \n",
    "    test_target = (test_input - 0.5).norm(p=2, dim=1)\n",
    "    test_target[test_target > dist] = 1\n",
    "    test_target[test_target <= dist] = 0\n",
    "    test_target = 1 - test_target\n",
    "    \n",
    "    return train_input, test_input, train_target, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, test_input, test_target, n_epoch):\n",
    "    \n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    nb_train_samples = train_input.shape[0]\n",
    "    nb_test_samples = test_input.shape[0]\n",
    "\n",
    "    #We change the output dimension in order to be similar to what pytorch does\n",
    "    train_target_2 = torch.empty(nb_train_samples, 2)\n",
    "    train_target_2[train_target == 0] = torch.tensor([1.0, 0.0])\n",
    "    train_target_2[train_target == 1] = torch.tensor([0.0, 1.0])\n",
    "    train_target = train_target_2\n",
    "\n",
    "    test_target_2 = torch.empty(nb_train_samples, 2)\n",
    "    test_target_2[test_target == 0] = torch.tensor([1.0, 0.0])\n",
    "    test_target_2[test_target == 1] = torch.tensor([0.0, 1.0])\n",
    "    test_target = test_target_2\n",
    "\n",
    "    zeta = 0.90\n",
    "    train_target = train_target * zeta\n",
    "    test_target = test_target * zeta\n",
    "\n",
    "    lr = 0.01/nb_train_samples\n",
    "    loss_criterion = loss_MSE()\n",
    "\n",
    "    for e in range(n_epoch):\n",
    "        model.grad_zero()\n",
    "        sum_loss = 0\n",
    "        nb_train_errors = 0\n",
    "        nb_test_errors = 0\n",
    "        for idx in range(nb_train_samples):\n",
    "            model_out = model(train_input[idx])\n",
    "            if train_target[idx].argmax() != model_out.argmax():  # Checking if prediction is correct\n",
    "                nb_train_errors += 1\n",
    "\n",
    "            loss = loss_criterion(model_out, train_target[idx])\n",
    "            loss_grad = loss_criterion.backward(model_out, train_target[idx])\n",
    "            model.backward(loss_grad)                         # Backward step\n",
    "\n",
    "            sum_loss = sum_loss + loss\n",
    "        #Gradient Step\n",
    "        paramlist, gradlist = model.param()\n",
    "        for i, (param, param_grad) in enumerate(zip(paramlist, gradlist)):\n",
    "            #print(param.max()/param_grad.max())\n",
    "            param -= lr*param_grad\n",
    "\n",
    "        for idx in range(nb_test_samples):\n",
    "            model_out = model(test_input[idx])\n",
    "            if test_target[idx].argmax() != model_out.argmax():\n",
    "                nb_test_errors += 1\n",
    "\n",
    "        train_error = nb_train_errors/nb_train_samples * 100\n",
    "        test_error = nb_test_errors/nb_test_samples * 100\n",
    "\n",
    "        print(\"{:d} Loss= {:.02f}  Train error = {:.02f}%, Test error = {:.02f}%\" \\\n",
    "              .format(e+1, sum_loss, train_error, test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, test_input, train_target, test_target = generate_data(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = Linear(2, 25)\n",
    "R1 = relu()\n",
    "L2 = Linear(25, 2)\n",
    "R2 = relu()\n",
    "\n",
    "model = Sequential(L1, R1, L2, R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Loss= 80.67  Train error = 53.00%, Test error = 48.00%\n",
      "2 Loss= 79.97  Train error = 53.00%, Test error = 48.00%\n",
      "3 Loss= 79.30  Train error = 53.00%, Test error = 48.00%\n",
      "4 Loss= 78.66  Train error = 53.00%, Test error = 48.00%\n",
      "5 Loss= 78.04  Train error = 53.00%, Test error = 48.00%\n",
      "6 Loss= 77.45  Train error = 53.00%, Test error = 48.00%\n",
      "7 Loss= 76.88  Train error = 53.00%, Test error = 48.00%\n",
      "8 Loss= 76.33  Train error = 53.00%, Test error = 48.00%\n",
      "9 Loss= 75.80  Train error = 53.00%, Test error = 48.00%\n",
      "10 Loss= 75.30  Train error = 53.00%, Test error = 48.00%\n",
      "11 Loss= 74.81  Train error = 53.00%, Test error = 48.00%\n",
      "12 Loss= 74.35  Train error = 53.00%, Test error = 48.00%\n",
      "13 Loss= 73.90  Train error = 53.00%, Test error = 48.00%\n",
      "14 Loss= 73.47  Train error = 53.00%, Test error = 48.00%\n",
      "15 Loss= 73.06  Train error = 53.00%, Test error = 48.00%\n",
      "16 Loss= 72.66  Train error = 53.00%, Test error = 48.00%\n",
      "17 Loss= 72.28  Train error = 53.00%, Test error = 48.00%\n",
      "18 Loss= 71.92  Train error = 53.00%, Test error = 48.00%\n",
      "19 Loss= 71.57  Train error = 53.00%, Test error = 48.00%\n",
      "20 Loss= 71.23  Train error = 53.00%, Test error = 48.00%\n",
      "21 Loss= 70.91  Train error = 53.00%, Test error = 48.00%\n",
      "22 Loss= 70.60  Train error = 53.00%, Test error = 48.00%\n",
      "23 Loss= 70.30  Train error = 53.00%, Test error = 48.00%\n",
      "24 Loss= 70.01  Train error = 53.00%, Test error = 48.00%\n",
      "25 Loss= 69.74  Train error = 53.00%, Test error = 48.00%\n",
      "26 Loss= 69.47  Train error = 53.00%, Test error = 48.00%\n",
      "27 Loss= 69.22  Train error = 53.00%, Test error = 48.00%\n",
      "28 Loss= 68.98  Train error = 53.00%, Test error = 48.00%\n",
      "29 Loss= 68.74  Train error = 53.00%, Test error = 48.00%\n",
      "30 Loss= 68.52  Train error = 53.00%, Test error = 48.00%\n",
      "31 Loss= 68.30  Train error = 53.00%, Test error = 48.00%\n",
      "32 Loss= 68.10  Train error = 53.00%, Test error = 48.00%\n",
      "33 Loss= 67.90  Train error = 53.00%, Test error = 48.00%\n",
      "34 Loss= 67.71  Train error = 53.00%, Test error = 48.00%\n",
      "35 Loss= 67.52  Train error = 53.00%, Test error = 48.00%\n",
      "36 Loss= 67.35  Train error = 53.00%, Test error = 48.00%\n",
      "37 Loss= 67.18  Train error = 53.00%, Test error = 48.00%\n",
      "38 Loss= 67.02  Train error = 53.00%, Test error = 48.00%\n",
      "39 Loss= 66.86  Train error = 53.00%, Test error = 48.00%\n",
      "40 Loss= 66.71  Train error = 53.00%, Test error = 48.00%\n",
      "41 Loss= 66.57  Train error = 53.00%, Test error = 48.00%\n",
      "42 Loss= 66.43  Train error = 53.00%, Test error = 48.00%\n",
      "43 Loss= 66.30  Train error = 53.00%, Test error = 48.00%\n",
      "44 Loss= 66.17  Train error = 53.00%, Test error = 48.00%\n",
      "45 Loss= 66.05  Train error = 53.00%, Test error = 48.00%\n",
      "46 Loss= 65.93  Train error = 53.00%, Test error = 48.00%\n",
      "47 Loss= 65.82  Train error = 53.00%, Test error = 48.00%\n",
      "48 Loss= 65.71  Train error = 53.00%, Test error = 48.00%\n",
      "49 Loss= 65.61  Train error = 53.00%, Test error = 48.00%\n",
      "50 Loss= 65.51  Train error = 53.00%, Test error = 48.00%\n",
      "51 Loss= 65.41  Train error = 53.00%, Test error = 48.00%\n",
      "52 Loss= 65.32  Train error = 53.00%, Test error = 48.00%\n",
      "53 Loss= 65.23  Train error = 53.00%, Test error = 48.00%\n",
      "54 Loss= 65.15  Train error = 53.00%, Test error = 48.00%\n",
      "55 Loss= 65.07  Train error = 53.00%, Test error = 48.00%\n",
      "56 Loss= 64.99  Train error = 53.00%, Test error = 48.00%\n",
      "57 Loss= 64.91  Train error = 53.00%, Test error = 48.00%\n",
      "58 Loss= 64.84  Train error = 53.00%, Test error = 48.00%\n",
      "59 Loss= 64.77  Train error = 53.00%, Test error = 48.00%\n",
      "60 Loss= 64.70  Train error = 53.00%, Test error = 48.00%\n",
      "61 Loss= 64.64  Train error = 53.00%, Test error = 48.00%\n",
      "62 Loss= 64.58  Train error = 53.00%, Test error = 48.00%\n",
      "63 Loss= 64.52  Train error = 53.00%, Test error = 48.00%\n",
      "64 Loss= 64.46  Train error = 53.00%, Test error = 48.00%\n",
      "65 Loss= 64.41  Train error = 53.00%, Test error = 48.00%\n",
      "66 Loss= 64.36  Train error = 53.00%, Test error = 48.00%\n",
      "67 Loss= 64.31  Train error = 53.00%, Test error = 48.00%\n",
      "68 Loss= 64.26  Train error = 53.00%, Test error = 48.00%\n",
      "69 Loss= 64.21  Train error = 53.00%, Test error = 48.00%\n",
      "70 Loss= 64.17  Train error = 53.00%, Test error = 48.00%\n",
      "71 Loss= 64.13  Train error = 53.00%, Test error = 48.00%\n",
      "72 Loss= 64.09  Train error = 53.00%, Test error = 48.00%\n",
      "73 Loss= 64.05  Train error = 53.00%, Test error = 48.00%\n",
      "74 Loss= 64.01  Train error = 53.00%, Test error = 48.00%\n",
      "75 Loss= 63.97  Train error = 53.00%, Test error = 48.00%\n",
      "76 Loss= 63.94  Train error = 53.00%, Test error = 48.00%\n",
      "77 Loss= 63.91  Train error = 53.00%, Test error = 48.00%\n",
      "78 Loss= 63.87  Train error = 53.00%, Test error = 48.00%\n",
      "79 Loss= 63.84  Train error = 53.00%, Test error = 48.00%\n",
      "80 Loss= 63.81  Train error = 53.00%, Test error = 48.00%\n",
      "81 Loss= 63.78  Train error = 53.00%, Test error = 48.00%\n",
      "82 Loss= 63.76  Train error = 53.00%, Test error = 48.00%\n",
      "83 Loss= 63.73  Train error = 53.00%, Test error = 48.00%\n",
      "84 Loss= 63.71  Train error = 53.00%, Test error = 48.00%\n",
      "85 Loss= 63.68  Train error = 53.00%, Test error = 48.00%\n",
      "86 Loss= 63.66  Train error = 53.00%, Test error = 48.00%\n",
      "87 Loss= 63.64  Train error = 53.00%, Test error = 48.00%\n",
      "88 Loss= 63.62  Train error = 53.00%, Test error = 48.00%\n",
      "89 Loss= 63.60  Train error = 53.00%, Test error = 48.00%\n",
      "90 Loss= 63.58  Train error = 53.00%, Test error = 48.00%\n",
      "91 Loss= 63.56  Train error = 53.00%, Test error = 48.00%\n",
      "92 Loss= 63.54  Train error = 53.00%, Test error = 48.00%\n",
      "93 Loss= 63.52  Train error = 53.00%, Test error = 48.00%\n",
      "94 Loss= 63.50  Train error = 53.00%, Test error = 48.00%\n",
      "95 Loss= 63.49  Train error = 53.00%, Test error = 48.00%\n",
      "96 Loss= 63.47  Train error = 53.00%, Test error = 48.00%\n",
      "97 Loss= 63.46  Train error = 53.00%, Test error = 48.00%\n",
      "98 Loss= 63.44  Train error = 53.00%, Test error = 48.00%\n",
      "99 Loss= 63.43  Train error = 53.00%, Test error = 48.00%\n",
      "100 Loss= 63.42  Train error = 53.00%, Test error = 48.00%\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_input, train_target, test_input, test_target, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
