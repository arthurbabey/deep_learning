{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/arthurbabey/deep_learning/blob/master/project_1/conv_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SL9c0tnT_9nV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IywA-9e2ADaO",
    "outputId": "41501795-b6c2-40c0-ec29-09211fe8031f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3jxszVuaBB5X"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/My Drive')\n",
    "import dlc_practical_prologue as prolog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bDhJtuEB_9ne"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def nb_errors(pred, truth):\n",
    "    \n",
    "    pred_class = pred.argmax(1)\n",
    "    return (pred_class - truth != 0).sum().item()\n",
    "\n",
    "    \n",
    "def train_model(model, train_input, train_target, test_input, test_target,  epochs=500, batch_size=100, lr=0.1):\n",
    "\n",
    "    torch.nn.init.xavier_uniform_(model.conv1.weight)\n",
    "    torch.nn.init.xavier_uniform_(model.conv2.weight)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    #scheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    best_accuracy = 0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "      \n",
    "        for b in range(0, train_input.size(0), batch_size):\n",
    "            output = model(train_input.narrow(0, b, batch_size))\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            loss = criterion(output, train_target.narrow(0, b, batch_size))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "\n",
    "        output_train = model(train_input)\n",
    "        model.eval()\n",
    "        output_test = model(test_input)\n",
    "        train_loss.append(criterion(output_train, train_target).item())\n",
    "        test_loss.append(criterion(output_test, test_target).item())\n",
    "        accuracy = 1 - nb_errors(output_test, test_target) / 1000\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_epoch = i+1\n",
    "        test_accuracy.append(accuracy)\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print('Epoch : ',i+1, '\\t', 'test loss :', test_loss[-1], '\\t', 'train loss', train_loss[-1])\n",
    "        \n",
    "    return train_loss, test_loss, test_accuracy, best_accuracy\n",
    "\n",
    "\n",
    "def nb_errors_10(pred, truth):\n",
    "    \n",
    "    pred_class = pred.view(-1, 2, 10).argmax(2).argmax(1)\n",
    "    return (pred_class - truth != 0).sum().item()\n",
    "\n",
    "\n",
    "def train_model_10(model, train_input, train_classes, test_input, test_target, test_classes,\\\n",
    "                epochs=250, batch_size=100, lr=0.1):\n",
    "    \n",
    "    torch.nn.init.xavier_uniform_(model.conv1.weight)\n",
    "    torch.nn.init.xavier_uniform_(model.conv2.weight)\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    best_accuracy = 0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        for b in range(0, train_input.size(0), batch_size):\n",
    "            output = model(train_input.narrow(0, b, batch_size))\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            labels = train_classes.narrow(0, b, batch_size)\n",
    "            loss = criterion(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        output_train = model(train_input)\n",
    "        output_test = model(test_input)\n",
    "        train_loss.append(criterion(output_train, train_classes).item())\n",
    "        test_loss.append(criterion(output_test, test_classes).item())\n",
    "        accuracy = 1 - nb_errors_10(output_test, test_target) / 1000\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_epoch = i+1\n",
    "        test_accuracy.append(accuracy)\n",
    "\n",
    "        #if i%100 == 0:\n",
    "            #print('Epoch : ',i+1, '\\t', 'test loss :', test_loss[-1], '\\t', 'train loss', train_loss[-1])\n",
    "       \n",
    "    return train_loss, test_loss, test_accuracy, best_accuracy, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4EdFrMwyHXuX"
   },
   "source": [
    "##Direct approch with 2 dim output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LSSrNaWt_9nm"
   },
   "outputs": [],
   "source": [
    "class BaseLine(nn.Module):\n",
    "    def __init__(self, nb_hidden = 50):\n",
    "        super(BaseLine, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 2, out_channels = 4, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(4, 8, kernel_size=2, stride = 1)\n",
    "        self.fc1 = nn.Linear(32, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 32)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class ConvNet_1(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(ConvNet_1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 2, out_channels = 4, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(4, 16, kernel_size=2, stride = 1)\n",
    "        self.fc1 = nn.Linear(64, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 64)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvNet_2(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(ConvNet_2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 2, out_channels = 8, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(8, 32, kernel_size=2, stride = 1)\n",
    "        self.fc1 = nn.Linear(128, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 128)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class ConvNet_3(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(ConvNet_3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 2, out_channels = 16, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(16, 64, kernel_size=2, stride = 1)\n",
    "        self.fc1 = nn.Linear(256, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 256)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class ConvNet_4(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(ConvNet_4, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 2, out_channels = 32, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(32, 128, kernel_size=2, stride = 1)\n",
    "        self.fc1 = nn.Linear(512, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 512)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "cxX79xt0ajQu",
    "outputId": "ffc11113-9742-4e2d-ec1e-7a0a56c6d385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1924 3660 7680 17256 42552\n",
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "baseline = BaseLine()\n",
    "\n",
    "model_1_0 = ConvNet_1(50)\n",
    "model_1_1 = ConvNet_1(100)\n",
    "model_1_2 = ConvNet_1(250)\n",
    "model_1_3 = ConvNet_1(1000)\n",
    "\n",
    "\n",
    "model_2_0 = ConvNet_2(50)\n",
    "model_2_1 = ConvNet_2(100)\n",
    "model_2_2 = ConvNet_2(250)\n",
    "model_2_3 = ConvNet_2(1000)\n",
    "\n",
    "\n",
    "model_3_0 = ConvNet_3(50)\n",
    "model_3_1 = ConvNet_3(100)\n",
    "model_3_2 = ConvNet_3(250)\n",
    "model_3_3 = ConvNet_3(1000)\n",
    "\n",
    "\n",
    "model_4_0 = ConvNet_4(50)\n",
    "model_4_1 = ConvNet_4(100)\n",
    "model_4_2 = ConvNet_4(250)\n",
    "model_4_3 = ConvNet_4(1000)\n",
    "\n",
    "\n",
    "print(count_parameters(baseline),\n",
    "count_parameters(model_1_0),\n",
    "count_parameters(model_2_0),\n",
    "count_parameters(model_3_0),\n",
    "count_parameters(model_4_0))\n",
    "\n",
    "\n",
    "models = [baseline, model_1_0,model_1_1, model_1_2, model_1_3,\n",
    "          model_2_0,model_2_1, model_2_2, model_2_3,\n",
    "          model_3_0,model_3_1, model_3_2, model_3_3,\n",
    "          model_4_0,model_4_1, model_4_2, model_4_3]\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\") \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "\n",
    "for model in models:\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1gwTyWJ7ajVE",
    "outputId": "2744e699-14e8-4b8e-9b1b-16e586576936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 \t test loss : 6.2853312492370605 \t train loss 13.503097534179688\n",
      "Epoch :  101 \t test loss : 0.6586242914199829 \t train loss 0.6643292307853699\n",
      "Model 0 0.6739999999999999\n",
      "Epoch :  1 \t test loss : 3.8688979148864746 \t train loss 7.858476638793945\n",
      "Epoch :  101 \t test loss : 0.6413178443908691 \t train loss 0.6414110660552979\n",
      "Model 1 0.73\n",
      "Epoch :  1 \t test loss : 2.9207763671875 \t train loss 6.786711692810059\n",
      "Epoch :  101 \t test loss : 0.6381037831306458 \t train loss 0.6427411437034607\n",
      "Model 2 0.726\n",
      "Epoch :  1 \t test loss : 2.358273506164551 \t train loss 13.141362190246582\n",
      "Epoch :  101 \t test loss : 0.5760365128517151 \t train loss 0.619005024433136\n",
      "Model 3 0.773\n",
      "Epoch :  1 \t test loss : 3.1755950450897217 \t train loss 5.869510650634766\n",
      "Epoch :  101 \t test loss : 0.5555346608161926 \t train loss 0.5107285380363464\n",
      "Model 4 0.757\n",
      "Epoch :  1 \t test loss : 1.2428226470947266 \t train loss 2.9529402256011963\n",
      "Epoch :  101 \t test loss : 0.4755379855632782 \t train loss 0.47001850605010986\n",
      "Model 5 0.781\n",
      "Epoch :  1 \t test loss : 1.3325002193450928 \t train loss 3.8309926986694336\n",
      "Epoch :  101 \t test loss : 0.4666639566421509 \t train loss 0.49695611000061035\n",
      "Model 6 0.7969999999999999\n",
      "Epoch :  1 \t test loss : 4.062692642211914 \t train loss 7.406530857086182\n",
      "Epoch :  101 \t test loss : 0.49058547616004944 \t train loss 0.5100030303001404\n",
      "Model 7 0.8009999999999999\n",
      "Epoch :  1 \t test loss : 11.3212251663208 \t train loss 14.244855880737305\n",
      "Epoch :  101 \t test loss : 0.4883843660354614 \t train loss 0.4823830723762512\n",
      "Model 8 0.817\n",
      "Epoch :  1 \t test loss : 0.9352136850357056 \t train loss 2.1992006301879883\n",
      "Epoch :  101 \t test loss : 0.4164541959762573 \t train loss 0.36339977383613586\n",
      "Model 9 0.8200000000000001\n",
      "Epoch :  1 \t test loss : 0.9415990710258484 \t train loss 2.7520575523376465\n",
      "Epoch :  101 \t test loss : 0.3982756733894348 \t train loss 0.3274487555027008\n",
      "Model 10 0.844\n",
      "Epoch :  1 \t test loss : 3.215359926223755 \t train loss 4.726476669311523\n",
      "Epoch :  101 \t test loss : 0.40314605832099915 \t train loss 0.28624871373176575\n",
      "Model 11 0.832\n",
      "Epoch :  1 \t test loss : 1.100096344947815 \t train loss 4.487496852874756\n",
      "Epoch :  101 \t test loss : 0.46480968594551086 \t train loss 0.28657567501068115\n",
      "Model 12 0.823\n",
      "Epoch :  1 \t test loss : 0.6890136003494263 \t train loss 0.7438783049583435\n",
      "Epoch :  101 \t test loss : 0.5461989045143127 \t train loss 0.22242525219917297\n",
      "Model 13 0.834\n",
      "Epoch :  1 \t test loss : 1.1745563745498657 \t train loss 1.6787680387496948\n",
      "Epoch :  101 \t test loss : 0.5466623306274414 \t train loss 0.14430168271064758\n",
      "Model 14 0.832\n",
      "Epoch :  1 \t test loss : 1.3787140846252441 \t train loss 2.122763156890869\n",
      "Epoch :  101 \t test loss : 0.4785653352737427 \t train loss 0.13352052867412567\n",
      "Model 15 0.841\n",
      "Epoch :  1 \t test loss : 4.005240440368652 \t train loss 4.274437427520752\n",
      "Epoch :  101 \t test loss : 0.5176194906234741 \t train loss 0.1272151917219162\n",
      "Model 16 0.844\n",
      "Epoch :  1 \t test loss : 0.6951596140861511 \t train loss 0.7484408617019653\n",
      "Epoch :  101 \t test loss : 0.5988609790802002 \t train loss 0.6016139984130859\n",
      "Model 0 0.6970000000000001\n",
      "Epoch :  1 \t test loss : 0.7206041812896729 \t train loss 0.938355028629303\n",
      "Epoch :  101 \t test loss : 0.5406804084777832 \t train loss 0.5827943682670593\n",
      "Model 1 0.744\n",
      "Epoch :  1 \t test loss : 0.6906734704971313 \t train loss 0.7415793538093567\n",
      "Epoch :  101 \t test loss : 0.5204622149467468 \t train loss 0.5182037949562073\n",
      "Model 2 0.767\n",
      "Epoch :  1 \t test loss : 0.713391900062561 \t train loss 0.7811852097511292\n",
      "Epoch :  101 \t test loss : 0.5621224045753479 \t train loss 0.51555997133255\n",
      "Model 3 0.738\n",
      "Epoch :  1 \t test loss : 1.5798332691192627 \t train loss 3.9642081260681152\n",
      "Epoch :  101 \t test loss : 0.6129868030548096 \t train loss 0.59609454870224\n",
      "Model 4 0.723\n",
      "Epoch :  1 \t test loss : 0.7074303030967712 \t train loss 0.8285222053527832\n",
      "Epoch :  101 \t test loss : 0.5859892964363098 \t train loss 0.6015363931655884\n",
      "Model 5 0.767\n",
      "Epoch :  1 \t test loss : 0.7103058099746704 \t train loss 0.7823035717010498\n",
      "Epoch :  101 \t test loss : 0.5300472974777222 \t train loss 0.4991491734981537\n",
      "Model 6 0.763\n",
      "Epoch :  1 \t test loss : 0.6857866048812866 \t train loss 0.726742148399353\n",
      "Epoch :  101 \t test loss : 0.49411270022392273 \t train loss 0.41185784339904785\n",
      "Model 7 0.772\n",
      "Epoch :  1 \t test loss : 0.9176251292228699 \t train loss 1.4042469263076782\n",
      "Epoch :  101 \t test loss : 0.47790995240211487 \t train loss 0.42868879437446594\n",
      "Model 8 0.789\n",
      "Epoch :  1 \t test loss : 0.6977242231369019 \t train loss 0.7300341725349426\n",
      "Epoch :  101 \t test loss : 0.6950656771659851 \t train loss 0.6851860284805298\n",
      "Model 9 0.524\n",
      "Epoch :  1 \t test loss : 0.7006317377090454 \t train loss 0.7296364307403564\n",
      "Epoch :  101 \t test loss : 0.5092217326164246 \t train loss 0.44341063499450684\n",
      "Model 10 0.777\n",
      "Epoch :  1 \t test loss : 0.6898871064186096 \t train loss 0.7243985533714294\n",
      "Epoch :  101 \t test loss : 0.4759756922721863 \t train loss 0.2773783802986145\n",
      "Model 11 0.8089999999999999\n",
      "Epoch :  1 \t test loss : 0.71542888879776 \t train loss 1.235504388809204\n",
      "Epoch :  101 \t test loss : 0.46853116154670715 \t train loss 0.3313898742198944\n",
      "Model 12 0.81\n",
      "Epoch :  1 \t test loss : 0.709401547908783 \t train loss 0.7358301877975464\n",
      "Epoch :  101 \t test loss : 0.5714497566223145 \t train loss 0.5141831040382385\n",
      "Model 13 0.79\n",
      "Epoch :  1 \t test loss : 0.6929779648780823 \t train loss 0.7611854672431946\n",
      "Epoch :  101 \t test loss : 0.572913646697998 \t train loss 0.5567554235458374\n",
      "Model 14 0.795\n",
      "Epoch :  1 \t test loss : 0.6875089406967163 \t train loss 0.7234514355659485\n",
      "Epoch :  101 \t test loss : 0.6600717902183533 \t train loss 0.13699233531951904\n",
      "Model 15 0.798\n",
      "Epoch :  1 \t test loss : 0.6800684332847595 \t train loss 0.9090192914009094\n",
      "Epoch :  101 \t test loss : 0.5409492254257202 \t train loss 0.2595062851905823\n",
      "Model 16 0.8109999999999999\n",
      "Epoch :  1 \t test loss : 0.6897177696228027 \t train loss 0.7118756771087646\n",
      "Epoch :  101 \t test loss : 0.5963677167892456 \t train loss 0.602893054485321\n",
      "Model 0 0.6839999999999999\n",
      "Epoch :  1 \t test loss : 0.7095283269882202 \t train loss 0.7061783075332642\n",
      "Epoch :  101 \t test loss : 0.5529261231422424 \t train loss 0.5611085295677185\n",
      "Model 1 0.739\n",
      "Epoch :  1 \t test loss : 0.7037356495857239 \t train loss 0.7897289991378784\n",
      "Epoch :  101 \t test loss : 0.5741329193115234 \t train loss 0.5685644745826721\n",
      "Model 2 0.73\n",
      "Epoch :  1 \t test loss : 0.7020877599716187 \t train loss 1.0162640810012817\n",
      "Epoch :  101 \t test loss : 0.598264217376709 \t train loss 0.6310828328132629\n",
      "Model 3 0.742\n",
      "Epoch :  1 \t test loss : 0.6680678129196167 \t train loss 0.7904535531997681\n",
      "Epoch :  101 \t test loss : 0.5813137888908386 \t train loss 0.56819087266922\n",
      "Model 4 0.7130000000000001\n",
      "Epoch :  1 \t test loss : 0.692964494228363 \t train loss 0.703227698802948\n",
      "Epoch :  101 \t test loss : 0.5070005655288696 \t train loss 0.5284106731414795\n",
      "Model 5 0.785\n",
      "Epoch :  1 \t test loss : 0.69252610206604 \t train loss 0.7236832976341248\n",
      "Epoch :  101 \t test loss : 0.4825812578201294 \t train loss 0.5096666216850281\n",
      "Model 6 0.774\n",
      "Epoch :  1 \t test loss : 0.6878392100334167 \t train loss 0.702453076839447\n",
      "Epoch :  101 \t test loss : 0.5046136379241943 \t train loss 0.5236111283302307\n",
      "Model 7 0.798\n",
      "Epoch :  1 \t test loss : 0.7691842913627625 \t train loss 1.513993263244629\n",
      "Epoch :  101 \t test loss : 0.539071261882782 \t train loss 0.6006753444671631\n",
      "Model 8 0.795\n",
      "Epoch :  1 \t test loss : 0.6891193985939026 \t train loss 0.6871404051780701\n",
      "Epoch :  101 \t test loss : 0.4818708896636963 \t train loss 0.3986063003540039\n",
      "Model 9 0.8029999999999999\n",
      "Epoch :  1 \t test loss : 0.7579532861709595 \t train loss 0.7758970260620117\n",
      "Epoch :  101 \t test loss : 0.46351170539855957 \t train loss 0.3371972441673279\n",
      "Model 10 0.808\n",
      "Epoch :  1 \t test loss : 0.68593829870224 \t train loss 0.8372982144355774\n",
      "Epoch :  101 \t test loss : 0.4777859151363373 \t train loss 0.4967886209487915\n",
      "Model 11 0.815\n",
      "Epoch :  1 \t test loss : 0.6917852163314819 \t train loss 0.7431043982505798\n",
      "Epoch :  101 \t test loss : 0.42994916439056396 \t train loss 0.2989554703235626\n",
      "Model 12 0.815\n",
      "Epoch :  1 \t test loss : 0.7449345588684082 \t train loss 0.7540037631988525\n",
      "Epoch :  101 \t test loss : 0.4673249423503876 \t train loss 0.2623502016067505\n",
      "Model 13 0.8220000000000001\n",
      "Epoch :  1 \t test loss : 0.6978780627250671 \t train loss 0.9302821755409241\n",
      "Epoch :  101 \t test loss : 0.428645521402359 \t train loss 0.2675204277038574\n",
      "Model 14 0.825\n",
      "Epoch :  1 \t test loss : 0.6848204135894775 \t train loss 0.7422941327095032\n",
      "Epoch :  101 \t test loss : 0.46264010667800903 \t train loss 0.20395511388778687\n",
      "Model 15 0.821\n",
      "Epoch :  1 \t test loss : 0.6972548365592957 \t train loss 0.7225112318992615\n",
      "Epoch :  101 \t test loss : 0.5430834889411926 \t train loss 0.1830047070980072\n",
      "Model 16 0.815\n",
      "Epoch :  1 \t test loss : 0.7082094550132751 \t train loss 0.8468824625015259\n",
      "Epoch :  101 \t test loss : 0.6211399435997009 \t train loss 0.6341274976730347\n",
      "Model 0 0.73\n",
      "Epoch :  1 \t test loss : 0.6873117685317993 \t train loss 0.730394721031189\n",
      "Epoch :  101 \t test loss : 0.5680500864982605 \t train loss 0.5810384750366211\n",
      "Model 1 0.739\n",
      "Epoch :  1 \t test loss : 0.6999409794807434 \t train loss 0.771334707736969\n",
      "Epoch :  101 \t test loss : 0.5677890181541443 \t train loss 0.5856053233146667\n",
      "Model 2 0.755\n",
      "Epoch :  1 \t test loss : 0.6886144876480103 \t train loss 0.6975785493850708\n",
      "Epoch :  101 \t test loss : 0.5748252272605896 \t train loss 0.5365173816680908\n",
      "Model 3 0.745\n",
      "Epoch :  1 \t test loss : 0.6873170733451843 \t train loss 1.050119161605835\n",
      "Epoch :  101 \t test loss : 0.6264001131057739 \t train loss 0.6069262027740479\n",
      "Model 4 0.754\n",
      "Epoch :  1 \t test loss : 0.6914152503013611 \t train loss 0.7052545547485352\n",
      "Epoch :  101 \t test loss : 0.4771387279033661 \t train loss 0.438751757144928\n",
      "Model 5 0.792\n",
      "Epoch :  1 \t test loss : 0.6954670548439026 \t train loss 0.7101484537124634\n",
      "Epoch :  101 \t test loss : 0.4936254620552063 \t train loss 0.45651599764823914\n",
      "Model 6 0.782\n",
      "Epoch :  1 \t test loss : 0.6932107210159302 \t train loss 0.7367691397666931\n",
      "Epoch :  101 \t test loss : 0.4841667711734772 \t train loss 0.46696898341178894\n",
      "Model 7 0.785\n",
      "Epoch :  1 \t test loss : 0.6998573541641235 \t train loss 0.784235954284668\n",
      "Epoch :  101 \t test loss : 0.4906964898109436 \t train loss 0.48174434900283813\n",
      "Model 8 0.781\n",
      "Epoch :  1 \t test loss : 0.6907363533973694 \t train loss 0.7295885682106018\n",
      "Epoch :  101 \t test loss : 0.5249338746070862 \t train loss 0.5791121125221252\n",
      "Model 9 0.8029999999999999\n",
      "Epoch :  1 \t test loss : 0.7542874813079834 \t train loss 0.7803186178207397\n",
      "Epoch :  101 \t test loss : 0.47921693325042725 \t train loss 0.3416716754436493\n",
      "Model 10 0.806\n",
      "Epoch :  1 \t test loss : 0.7043699026107788 \t train loss 0.8096423745155334\n",
      "Epoch :  101 \t test loss : 0.4740470349788666 \t train loss 0.4578898549079895\n",
      "Model 11 0.791\n",
      "Epoch :  1 \t test loss : 0.6884483695030212 \t train loss 0.7824080586433411\n",
      "Epoch :  101 \t test loss : 0.4341776371002197 \t train loss 0.340789258480072\n",
      "Model 12 0.8180000000000001\n",
      "Epoch :  1 \t test loss : 0.7632662057876587 \t train loss 0.7639871835708618\n",
      "Epoch :  101 \t test loss : 0.6073247194290161 \t train loss 0.15568585693836212\n",
      "Model 13 0.825\n",
      "Epoch :  1 \t test loss : 0.7942109704017639 \t train loss 0.7739549875259399\n",
      "Epoch :  101 \t test loss : 0.5477601885795593 \t train loss 0.1684155911207199\n",
      "Model 14 0.8280000000000001\n",
      "Epoch :  1 \t test loss : 0.7174076437950134 \t train loss 0.770937442779541\n",
      "Epoch :  101 \t test loss : 0.524899959564209 \t train loss 0.2396778017282486\n",
      "Model 15 0.8200000000000001\n",
      "Epoch :  1 \t test loss : 0.7154856324195862 \t train loss 0.758158802986145\n",
      "Epoch :  101 \t test loss : 0.47978901863098145 \t train loss 0.35318002104759216\n",
      "Model 16 0.8220000000000001\n",
      "It took 6.096552491188049 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "epochs = 200\n",
    "accuracies = torch.empty(17, 4, dtype=torch.float)\n",
    "\n",
    "for i in range(4):\n",
    "    train_input, train_target, train_classes, test_input, test_target, test_classes = prolog.generate_pair_sets(1000)\n",
    "    train_input = train_input.cuda()\n",
    "    train_target = train_target.cuda()\n",
    "    test_input = test_input.cuda()\n",
    "    test_target = test_target.cuda()\n",
    "\n",
    "    for j in range(17):\n",
    "        _, _, _, best_accuracy = train_model(models[j], train_input, train_target, test_input,\\\n",
    "                                             test_target, epochs=epochs, lr = 0.08)\n",
    "        print('Model', j , best_accuracy)\n",
    "        accuracies[j][i] = best_accuracy\n",
    "\n",
    "\n",
    "minute = (time.time()-start) / 60\n",
    "print('It took', minute, 'minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "EbNIdpP_ajXU",
    "outputId": "07b86aea-7fa3-43c3-bb63-33741a0b7d49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6963, 0.7380, 0.7445, 0.7495, 0.7368, 0.7813, 0.7790, 0.7890, 0.7955,\n",
       "        0.7375, 0.8088, 0.8118, 0.8165, 0.8178, 0.8200, 0.8200, 0.8230])"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PS6Nd8QyajZr",
    "outputId": "f73779cf-ccf5-4a2e-f7f2-db9ec2e8ad6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0244, 0.0058, 0.0197, 0.0159, 0.0221, 0.0105, 0.0143, 0.0133, 0.0154,\n",
       "        0.1426, 0.0274, 0.0169, 0.0054, 0.0192, 0.0169, 0.0176, 0.0147])"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.std(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U_Z_Dxvf42Ui"
   },
   "source": [
    "Model with bigger amount of parameters seems to work better, let's try with even more trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3FLEpwOtajd4"
   },
   "outputs": [],
   "source": [
    "class bigConvNet_1(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(bigConvNet_1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 2, out_channels = 64, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=2, stride = 1)\n",
    "        self.fc1 = nn.Linear(4*128, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 4*128)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class bigConvNet_2(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(bigConvNet_2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 2, out_channels = 64, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(64, 256, kernel_size=2, stride = 1)\n",
    "        self.fc1 = nn.Linear(4*256, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 4*256)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class bigConvNet_3(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(bigConvNet_3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 2, out_channels = 100, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(100, 200, kernel_size=2, stride = 1)\n",
    "        self.fc1 = nn.Linear(4*200, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 4*200)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class bigConvNet_4(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(bigConvNet_4, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 2, out_channels = 128, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(128, 1024, kernel_size=2, stride = 1)\n",
    "        self.fc1 = nn.Linear(4*1024, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 4*1024)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "wl9RL-BH8wTv",
    "outputId": "c4b2b4b1-64d1-4450-f423-f204acc372b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1924 59224 117720 121252 731416\n",
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "baseline = BaseLine()\n",
    "\n",
    "model_1_0 = bigConvNet_1(50)\n",
    "model_1_1 = bigConvNet_1(100)\n",
    "model_1_2 = bigConvNet_1(500)\n",
    "model_1_3 = bigConvNet_1(1000)\n",
    "\n",
    "\n",
    "model_2_0 = bigConvNet_2(50)\n",
    "model_2_1 = bigConvNet_2(100)\n",
    "model_2_2 = bigConvNet_2(500)\n",
    "model_2_3 = bigConvNet_2(1000)\n",
    "\n",
    "\n",
    "model_3_0 = bigConvNet_3(50)\n",
    "model_3_1 = bigConvNet_3(100)\n",
    "model_3_2 = bigConvNet_3(500)\n",
    "model_3_3 = bigConvNet_3(1000)\n",
    "\n",
    "\n",
    "\n",
    "model_4_0 = bigConvNet_4(50)\n",
    "model_4_1 = bigConvNet_4(100)\n",
    "model_4_2 = bigConvNet_4(500)\n",
    "model_4_3 = bigConvNet_4(1000)\n",
    "\n",
    "\n",
    "\n",
    "print(count_parameters(baseline),\n",
    "count_parameters(model_1_0),\n",
    "count_parameters(model_2_0),\n",
    "count_parameters(model_3_0),\n",
    "count_parameters(model_4_0))\n",
    "\n",
    "models = [baseline, model_1_0,model_1_1, model_1_2, model_1_3,\n",
    "          model_2_0,model_2_1, model_2_2, model_2_3,\n",
    "          model_3_0,model_3_1, model_3_2, model_3_3,\n",
    "          model_4_0,model_4_1, model_4_2, model_4_3]\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\") \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "\n",
    "for model in models:\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "e0TSAG038wYN",
    "outputId": "712717b5-55fc-429f-bd70-963d2c14fef5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 \t test loss : 1.9121816158294678 \t train loss 8.359524726867676\n",
      "Epoch :  101 \t test loss : 0.6508594155311584 \t train loss 0.6512908935546875\n",
      "Model 1 0.712\n",
      "Epoch :  1 \t test loss : 0.6660023927688599 \t train loss 0.7208255529403687\n",
      "Epoch :  101 \t test loss : 0.7267429828643799 \t train loss 0.08745211362838745\n",
      "Model 2 0.832\n",
      "Epoch :  1 \t test loss : 0.7780104875564575 \t train loss 1.3403571844100952\n",
      "Epoch :  101 \t test loss : 0.5561511516571045 \t train loss 0.11301194876432419\n",
      "Model 3 0.836\n",
      "Epoch :  1 \t test loss : 1.9883372783660889 \t train loss 2.2667627334594727\n",
      "Epoch :  101 \t test loss : 0.5385529398918152 \t train loss 0.08430512249469757\n",
      "Model 4 0.831\n",
      "Epoch :  1 \t test loss : 1.8702545166015625 \t train loss 3.328080415725708\n",
      "Epoch :  101 \t test loss : 0.5841556787490845 \t train loss 0.1050393208861351\n",
      "Model 5 0.835\n",
      "Epoch :  1 \t test loss : 0.6958605051040649 \t train loss 0.6937540173530579\n",
      "Epoch :  101 \t test loss : 0.6869242191314697 \t train loss 0.6835947036743164\n",
      "Model 6 0.5569999999999999\n",
      "Epoch :  1 \t test loss : 0.7135426998138428 \t train loss 0.7465212345123291\n",
      "Epoch :  101 \t test loss : 0.705646812915802 \t train loss 0.08465899527072906\n",
      "Model 7 0.838\n",
      "Epoch :  1 \t test loss : 1.2269556522369385 \t train loss 1.3724277019500732\n",
      "Epoch :  101 \t test loss : 0.6846024394035339 \t train loss 0.0630955770611763\n",
      "Model 8 0.841\n",
      "Epoch :  1 \t test loss : 0.7823039889335632 \t train loss 0.9186146855354309\n",
      "Epoch :  101 \t test loss : 0.6454588174819946 \t train loss 0.08270978182554245\n",
      "Model 9 0.825\n",
      "Epoch :  1 \t test loss : 0.6857250332832336 \t train loss 0.7234064936637878\n",
      "Epoch :  101 \t test loss : 0.6154513359069824 \t train loss 0.11643538624048233\n",
      "Model 10 0.84\n",
      "Epoch :  1 \t test loss : 0.7770092487335205 \t train loss 1.0039496421813965\n",
      "Epoch :  101 \t test loss : 0.8151095509529114 \t train loss 0.08654467016458511\n",
      "Model 11 0.842\n",
      "Epoch :  1 \t test loss : 0.6965746879577637 \t train loss 1.3516305685043335\n",
      "Epoch :  101 \t test loss : 0.6842540502548218 \t train loss 0.04224846512079239\n",
      "Model 12 0.844\n",
      "Epoch :  1 \t test loss : 0.8870585560798645 \t train loss 1.239302158355713\n",
      "Epoch :  101 \t test loss : 0.7148395776748657 \t train loss 0.05383630469441414\n",
      "Model 13 0.833\n",
      "Epoch :  1 \t test loss : 0.6861557960510254 \t train loss 0.6856936812400818\n",
      "Epoch :  101 \t test loss : 0.6869311332702637 \t train loss 0.6835947036743164\n",
      "Model 14 0.5569999999999999\n",
      "Epoch :  1 \t test loss : 0.6673312783241272 \t train loss 0.7114249467849731\n",
      "Epoch :  101 \t test loss : 0.7055555582046509 \t train loss 0.06613575667142868\n",
      "Model 15 0.837\n",
      "Epoch :  1 \t test loss : 0.8074781894683838 \t train loss 0.874192476272583\n",
      "Epoch :  101 \t test loss : 0.7197198271751404 \t train loss 0.050895944237709045\n",
      "Model 16 0.8220000000000001\n",
      "Epoch :  1 \t test loss : 0.696774423122406 \t train loss 0.7076934576034546\n",
      "Epoch :  101 \t test loss : 0.7060771584510803 \t train loss 0.07206970453262329\n",
      "Model 17 0.819\n",
      "Epoch :  1 \t test loss : 0.7202951908111572 \t train loss 1.2164214849472046\n",
      "Epoch :  101 \t test loss : 0.6263298988342285 \t train loss 0.6484208703041077\n",
      "Model 1 0.719\n",
      "Epoch :  1 \t test loss : 0.6859941482543945 \t train loss 0.6906580328941345\n",
      "Epoch :  101 \t test loss : 0.4906810522079468 \t train loss 0.24692106246948242\n",
      "Model 2 0.825\n",
      "Epoch :  1 \t test loss : 0.693255603313446 \t train loss 0.6996989250183105\n",
      "Epoch :  101 \t test loss : 0.6837241053581238 \t train loss 0.6867510080337524\n",
      "Model 3 0.5700000000000001\n",
      "Epoch :  1 \t test loss : 0.6770363450050354 \t train loss 0.7596938610076904\n",
      "Epoch :  101 \t test loss : 0.6458569765090942 \t train loss 0.0692681223154068\n",
      "Model 4 0.831\n",
      "Epoch :  1 \t test loss : 0.6683177351951599 \t train loss 0.7634340524673462\n",
      "Epoch :  101 \t test loss : 0.6948136687278748 \t train loss 0.07968782633543015\n",
      "Model 5 0.838\n",
      "Epoch :  1 \t test loss : 0.6833512187004089 \t train loss 0.6892715096473694\n",
      "Epoch :  101 \t test loss : 0.6837088465690613 \t train loss 0.686862051486969\n",
      "Model 6 0.5700000000000001\n",
      "Epoch :  1 \t test loss : 0.7147907614707947 \t train loss 0.7132765650749207\n",
      "Epoch :  101 \t test loss : 0.6837247610092163 \t train loss 0.6868621110916138\n",
      "Model 7 0.5700000000000001\n",
      "Epoch :  1 \t test loss : 0.6938191652297974 \t train loss 0.7142057418823242\n",
      "Epoch :  101 \t test loss : 0.7569507956504822 \t train loss 0.05551592633128166\n",
      "Model 8 0.843\n",
      "Epoch :  1 \t test loss : 0.6887584328651428 \t train loss 0.70845627784729\n",
      "Epoch :  101 \t test loss : 0.6304494738578796 \t train loss 0.08727696537971497\n",
      "Model 9 0.84\n",
      "Epoch :  1 \t test loss : 0.6846774220466614 \t train loss 0.689134955406189\n",
      "Epoch :  101 \t test loss : 0.6837092041969299 \t train loss 0.6868619918823242\n",
      "Model 10 0.5700000000000001\n",
      "Epoch :  1 \t test loss : 0.7005806565284729 \t train loss 0.7027969360351562\n",
      "Epoch :  101 \t test loss : 0.6385084390640259 \t train loss 0.1389494389295578\n",
      "Model 11 0.85\n",
      "Epoch :  1 \t test loss : 0.7150722146034241 \t train loss 0.8482070565223694\n",
      "Epoch :  101 \t test loss : 0.6863086223602295 \t train loss 0.06157010421156883\n",
      "Model 12 0.849\n",
      "Epoch :  1 \t test loss : 0.8764578104019165 \t train loss 1.071847677230835\n",
      "Epoch :  101 \t test loss : 0.6812788844108582 \t train loss 0.10468550771474838\n",
      "Model 13 0.845\n",
      "Epoch :  1 \t test loss : 0.6833204030990601 \t train loss 0.6871713995933533\n",
      "Epoch :  101 \t test loss : 0.6837090849876404 \t train loss 0.6868622303009033\n",
      "Model 14 0.5700000000000001\n",
      "Epoch :  1 \t test loss : 0.7090675830841064 \t train loss 0.7066364288330078\n",
      "Epoch :  101 \t test loss : 0.6837161779403687 \t train loss 0.6868621110916138\n",
      "Model 15 0.5700000000000001\n",
      "Epoch :  1 \t test loss : 0.7238510251045227 \t train loss 1.3292824029922485\n",
      "Epoch :  101 \t test loss : 0.840361475944519 \t train loss 0.023424699902534485\n",
      "Model 16 0.841\n",
      "Epoch :  1 \t test loss : 2.7352993488311768 \t train loss 2.6316497325897217\n",
      "Epoch :  101 \t test loss : 0.9228144288063049 \t train loss 0.03647575527429581\n",
      "Model 17 0.846\n",
      "Epoch :  1 \t test loss : 0.6968451738357544 \t train loss 0.7616267204284668\n",
      "Epoch :  101 \t test loss : 0.6232826113700867 \t train loss 0.6353312134742737\n",
      "Model 1 0.706\n",
      "Epoch :  1 \t test loss : 0.6923936605453491 \t train loss 0.6868866086006165\n",
      "Epoch :  101 \t test loss : 0.5519292950630188 \t train loss 0.17429935932159424\n",
      "Model 2 0.815\n",
      "Epoch :  1 \t test loss : 0.6934130787849426 \t train loss 0.6863026022911072\n",
      "Epoch :  101 \t test loss : 0.6931604146957397 \t train loss 0.6881386041641235\n",
      "Model 3 0.525\n",
      "Epoch :  1 \t test loss : 0.687024712562561 \t train loss 0.7363031506538391\n",
      "Epoch :  101 \t test loss : 0.5930942296981812 \t train loss 0.19396676123142242\n",
      "Model 4 0.821\n",
      "Epoch :  1 \t test loss : 0.6850001811981201 \t train loss 0.7549794316291809\n",
      "Epoch :  101 \t test loss : 0.6197426319122314 \t train loss 0.12900450825691223\n",
      "Model 5 0.8160000000000001\n",
      "Epoch :  1 \t test loss : 0.6937798261642456 \t train loss 0.6882012486457825\n",
      "Epoch :  101 \t test loss : 0.6931607127189636 \t train loss 0.6881387829780579\n",
      "Model 6 0.525\n",
      "Epoch :  1 \t test loss : 0.693737268447876 \t train loss 0.6884124875068665\n",
      "Epoch :  101 \t test loss : 0.6931607127189636 \t train loss 0.6881387829780579\n",
      "Model 7 0.525\n",
      "Epoch :  1 \t test loss : 0.7037799954414368 \t train loss 0.698533296585083\n",
      "Epoch :  101 \t test loss : 0.6481164693832397 \t train loss 0.19272874295711517\n",
      "Model 8 0.813\n",
      "Epoch :  1 \t test loss : 0.6805748343467712 \t train loss 0.7436609268188477\n",
      "Epoch :  101 \t test loss : 0.5938755869865417 \t train loss 0.20372262597084045\n",
      "Model 9 0.825\n",
      "Epoch :  1 \t test loss : 0.6936686635017395 \t train loss 0.6877639889717102\n",
      "Epoch :  101 \t test loss : 0.6931605339050293 \t train loss 0.6881386637687683\n",
      "Model 10 0.525\n",
      "Epoch :  1 \t test loss : 0.6920201182365417 \t train loss 0.8569715619087219\n",
      "Epoch :  101 \t test loss : 0.7458965182304382 \t train loss 0.06647282838821411\n",
      "Model 11 0.817\n",
      "Epoch :  1 \t test loss : 0.6918697953224182 \t train loss 0.6828333139419556\n",
      "Epoch :  101 \t test loss : 0.8229802846908569 \t train loss 0.05192428454756737\n",
      "Model 12 0.8160000000000001\n",
      "Epoch :  1 \t test loss : 0.6901236176490784 \t train loss 0.7669265866279602\n",
      "Epoch :  101 \t test loss : 0.6825454831123352 \t train loss 0.08176720887422562\n",
      "Model 13 0.8220000000000001\n",
      "Epoch :  1 \t test loss : 0.6937799453735352 \t train loss 0.6882013082504272\n",
      "Epoch :  101 \t test loss : 0.6931608319282532 \t train loss 0.6881388425827026\n",
      "Model 14 0.525\n",
      "Epoch :  1 \t test loss : 0.6937790513038635 \t train loss 0.6882010698318481\n",
      "Epoch :  101 \t test loss : 0.6931607127189636 \t train loss 0.6881387829780579\n",
      "Model 15 0.525\n",
      "Epoch :  1 \t test loss : 1.2336788177490234 \t train loss 2.8108012676239014\n",
      "Epoch :  101 \t test loss : 0.7447776198387146 \t train loss 0.04622301831841469\n",
      "Model 16 0.825\n",
      "Epoch :  1 \t test loss : 1.361450433731079 \t train loss 2.6197822093963623\n",
      "Epoch :  101 \t test loss : 0.7631427049636841 \t train loss 0.04919116571545601\n",
      "Model 17 0.8240000000000001\n",
      "Epoch :  1 \t test loss : 0.6765326857566833 \t train loss 0.923860490322113\n",
      "Epoch :  101 \t test loss : 0.6557153463363647 \t train loss 0.6576400995254517\n",
      "Model 1 0.744\n",
      "Epoch :  1 \t test loss : 0.6956857442855835 \t train loss 0.763167142868042\n",
      "Epoch :  101 \t test loss : 0.5423184633255005 \t train loss 0.1475813388824463\n",
      "Model 2 0.819\n",
      "Epoch :  1 \t test loss : 0.6867808103561401 \t train loss 0.6894534826278687\n",
      "Epoch :  101 \t test loss : 0.6866636276245117 \t train loss 0.6873049139976501\n",
      "Model 3 0.5569999999999999\n",
      "Epoch :  1 \t test loss : 0.7086524367332458 \t train loss 0.7217378616333008\n",
      "Epoch :  101 \t test loss : 0.6613661050796509 \t train loss 0.13403315842151642\n",
      "Model 4 0.84\n",
      "Epoch :  1 \t test loss : 0.7203273177146912 \t train loss 0.7944788932800293\n",
      "Epoch :  101 \t test loss : 0.5766327381134033 \t train loss 0.10837362706661224\n",
      "Model 5 0.837\n",
      "Epoch :  1 \t test loss : 0.6867544651031494 \t train loss 0.6873480677604675\n",
      "Epoch :  101 \t test loss : 0.686652421951294 \t train loss 0.6873038411140442\n",
      "Model 6 0.5569999999999999\n",
      "Epoch :  1 \t test loss : 0.6867583394050598 \t train loss 0.6873505711555481\n",
      "Epoch :  101 \t test loss : 0.686652421951294 \t train loss 0.6873038411140442\n",
      "Model 7 0.5569999999999999\n",
      "Epoch :  1 \t test loss : 0.678586483001709 \t train loss 0.8397115468978882\n",
      "Epoch :  101 \t test loss : 0.5926492810249329 \t train loss 0.0820084735751152\n",
      "Model 8 0.837\n",
      "Epoch :  1 \t test loss : 0.7881284952163696 \t train loss 0.7725046277046204\n",
      "Epoch :  101 \t test loss : 0.5027927160263062 \t train loss 0.20969323813915253\n",
      "Model 9 0.833\n",
      "Epoch :  1 \t test loss : 0.6867576837539673 \t train loss 0.6873500943183899\n",
      "Epoch :  101 \t test loss : 0.6866525411605835 \t train loss 0.6873039603233337\n",
      "Model 10 0.5569999999999999\n",
      "Epoch :  1 \t test loss : 0.7602077722549438 \t train loss 0.7585342526435852\n",
      "Epoch :  101 \t test loss : 0.7436273097991943 \t train loss 0.08294017612934113\n",
      "Model 11 0.836\n",
      "Epoch :  1 \t test loss : 0.6587927937507629 \t train loss 0.8232815265655518\n",
      "Epoch :  101 \t test loss : 0.5847909450531006 \t train loss 0.09055816382169724\n",
      "Model 12 0.835\n",
      "Epoch :  1 \t test loss : 0.6278641223907471 \t train loss 1.001535177230835\n",
      "Epoch :  101 \t test loss : 0.46297428011894226 \t train loss 0.20159350335597992\n",
      "Model 13 0.835\n",
      "Epoch :  1 \t test loss : 0.6867547035217285 \t train loss 0.6873483061790466\n",
      "Epoch :  101 \t test loss : 0.6866525411605835 \t train loss 0.6873040199279785\n",
      "Model 14 0.5569999999999999\n",
      "Epoch :  1 \t test loss : 0.6867546439170837 \t train loss 0.6873482465744019\n",
      "Epoch :  101 \t test loss : 0.6866525411605835 \t train loss 0.6873039603233337\n",
      "Model 15 0.5569999999999999\n",
      "Epoch :  1 \t test loss : 1.2960669994354248 \t train loss 3.290189266204834\n",
      "Epoch :  101 \t test loss : 0.6477595567703247 \t train loss 0.05533967539668083\n",
      "Model 16 0.843\n",
      "Epoch :  1 \t test loss : 0.8678001761436462 \t train loss 1.1993528604507446\n",
      "Epoch :  101 \t test loss : 0.47059357166290283 \t train loss 0.19039884209632874\n",
      "Model 17 0.8280000000000001\n",
      "It took 11.888036433855692 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "epochs = 200\n",
    "accuracies = torch.empty(17, 4, dtype=torch.float)\n",
    "\n",
    "for i in range(4):\n",
    "    train_input, train_target, train_classes, test_input, test_target, test_classes = prolog.generate_pair_sets(1000)\n",
    "    if torch.cuda.is_available():\n",
    "      train_input = train_input.cuda()\n",
    "      train_target = train_target.cuda()\n",
    "      test_input = test_input.cuda()\n",
    "      test_target = test_target.cuda()\n",
    "\n",
    "    for j in range(17):\n",
    "        _, _, _, best_accuracy = train_model(models[j], train_input, train_target, test_input,\\\n",
    "                                             test_target, epochs=epochs, lr = 0.08)\n",
    "        print('Model', j+1 , best_accuracy)\n",
    "        accuracies[j][i] = best_accuracy\n",
    "\n",
    "\n",
    "minute = (time.time()-start) / 60\n",
    "print('It took', minute, 'minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "aKhWcXo08wcl",
    "outputId": "426cf802-b606-4a64-a6f4-7be77c0acf5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7203, 0.8228, 0.6220, 0.8307, 0.8315, 0.5522, 0.6225, 0.8335, 0.8307,\n",
       "        0.6230, 0.8363, 0.8360, 0.8338, 0.5522, 0.6223, 0.8328, 0.8292])"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "pa_swS9CBYOJ",
    "outputId": "8ebcd8a7-974e-4a70-b32e-46902d26f909"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0167, 0.0074, 0.1439, 0.0078, 0.0104, 0.0192, 0.1449, 0.0139, 0.0072,\n",
       "        0.1459, 0.0141, 0.0145, 0.0094, 0.0192, 0.1444, 0.0108, 0.0118])"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.std(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2elGpbhBjpP"
   },
   "source": [
    "Some model show bad result could be bc they need bigger dense layer due to their big number of parameter, but now it seems that increasing the number of parameters does not improve the performance. For further hyperparameter exploration i'll keep ConvNet_3, ConvNet_4, bigConvNet_1 which are the model that show the best accuracies with the less amount of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gxi1ME_SJXV-"
   },
   "source": [
    "##Direct approch with 10 dim output and deterministic comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VxwkH_8WFJS5"
   },
   "outputs": [],
   "source": [
    "class BaseLine(nn.Module):\n",
    "    def __init__(self, nb_hidden = 50):\n",
    "        super(BaseLine, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 4, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(4, 8, kernel_size=2, stride = 1)\n",
    "        self.fc1 = nn.Linear(32, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 32)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class ConvNet_1(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(ConvNet_1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 4, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(4, 16, kernel_size=2, stride = 1)\n",
    "        self.fc1 = nn.Linear(64, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 64)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvNet_2(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(ConvNet_2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(8, 32, kernel_size=2, stride = 1)\n",
    "        self.fc1 = nn.Linear(128, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 128)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class ConvNet_3(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(ConvNet_3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 16, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(16, 64, kernel_size=2, stride = 1)\n",
    "        self.fc1 = nn.Linear(256, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 256)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class ConvNet_4(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(ConvNet_4, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(32, 128, kernel_size=2, stride = 1)\n",
    "        self.fc1 = nn.Linear(512, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 512)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NxZ2eIttJskm",
    "outputId": "6d045f3b-7b50-4019-ad5a-51a3c65547f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2316 4052 8056 17600 42832\n"
     ]
    }
   ],
   "source": [
    "baseline = BaseLine()\n",
    "\n",
    "model_1_0 = ConvNet_1(50)\n",
    "model_1_1 = ConvNet_1(100)\n",
    "model_1_2 = ConvNet_1(250)\n",
    "model_1_3 = ConvNet_1(1000)\n",
    "\n",
    "\n",
    "model_2_0 = ConvNet_2(50)\n",
    "model_2_1 = ConvNet_2(100)\n",
    "model_2_2 = ConvNet_2(250)\n",
    "model_2_3 = ConvNet_2(1000)\n",
    "\n",
    "\n",
    "model_3_0 = ConvNet_3(50)\n",
    "model_3_1 = ConvNet_3(100)\n",
    "model_3_2 = ConvNet_3(250)\n",
    "model_3_3 = ConvNet_3(1000)\n",
    "\n",
    "\n",
    "model_4_0 = ConvNet_4(50)\n",
    "model_4_1 = ConvNet_4(100)\n",
    "model_4_2 = ConvNet_4(250)\n",
    "model_4_3 = ConvNet_4(1000)\n",
    "\n",
    "\n",
    "print(count_parameters(baseline),\n",
    "count_parameters(model_1_0),\n",
    "count_parameters(model_2_0),\n",
    "count_parameters(model_3_0),\n",
    "count_parameters(model_4_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0E1MMdC1CPAR",
    "outputId": "4f1bc903-e6f7-4c4a-fbfe-748b0a6185b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "models = [baseline, model_1_0,model_1_1, model_1_2, model_1_3,\n",
    "          model_2_0,model_2_1, model_2_2, model_2_3,\n",
    "          model_3_0,model_3_1, model_3_2, model_3_3,\n",
    "          model_4_0,model_4_1, model_4_2, model_4_3]\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\") \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "\n",
    "for model in models:\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "41OOU7dJ56iS",
    "outputId": "91250b7a-8105-4d2c-9f6b-facd50bae721"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 \t test loss : 11.122110366821289 \t train loss 10.812323570251465\n",
      "Epoch :  101 \t test loss : 1.231086015701294 \t train loss 1.2648133039474487\n",
      "Model 0 0.8069999999999999\n",
      "Epoch :  1 \t test loss : 9.258034706115723 \t train loss 9.429718017578125\n",
      "Epoch :  101 \t test loss : 1.3933097124099731 \t train loss 1.358246088027954\n",
      "Model 1 0.804\n",
      "Epoch :  1 \t test loss : 5.32855749130249 \t train loss 5.408300876617432\n",
      "Epoch :  101 \t test loss : 0.9879733324050903 \t train loss 0.9213743805885315\n",
      "Model 2 0.855\n",
      "Epoch :  1 \t test loss : 6.922041893005371 \t train loss 6.826251029968262\n",
      "Epoch :  101 \t test loss : 0.8935632109642029 \t train loss 0.796588122844696\n",
      "Model 3 0.874\n",
      "Epoch :  1 \t test loss : 6.663966655731201 \t train loss 6.437285900115967\n",
      "Epoch :  101 \t test loss : 0.8754478096961975 \t train loss 0.6853734254837036\n",
      "Model 4 0.854\n",
      "Epoch :  1 \t test loss : 2.6131930351257324 \t train loss 2.669412136077881\n",
      "Epoch :  101 \t test loss : 0.8481494188308716 \t train loss 0.7182619571685791\n",
      "Model 5 0.89\n",
      "Epoch :  1 \t test loss : 3.79123592376709 \t train loss 3.8132665157318115\n",
      "Epoch :  101 \t test loss : 0.6921468377113342 \t train loss 0.5659933686256409\n",
      "Model 6 0.917\n",
      "Epoch :  1 \t test loss : 5.8073344230651855 \t train loss 5.721575736999512\n",
      "Epoch :  101 \t test loss : 0.6460357308387756 \t train loss 0.43047580122947693\n",
      "Model 7 0.924\n",
      "Epoch :  1 \t test loss : 7.114464282989502 \t train loss 7.253687381744385\n",
      "Epoch :  101 \t test loss : 0.4973272383213043 \t train loss 0.2729778587818146\n",
      "Model 8 0.9359999999999999\n",
      "Epoch :  1 \t test loss : 2.313424587249756 \t train loss 2.309535503387451\n",
      "Epoch :  101 \t test loss : 0.5088611245155334 \t train loss 0.3479628562927246\n",
      "Model 9 0.9390000000000001\n",
      "Epoch :  1 \t test loss : 2.3405401706695557 \t train loss 2.3233397006988525\n",
      "Epoch :  101 \t test loss : 0.48043447732925415 \t train loss 0.3038587272167206\n",
      "Model 10 0.949\n",
      "Epoch :  1 \t test loss : 2.470391035079956 \t train loss 2.51070499420166\n",
      "Epoch :  101 \t test loss : 0.39005422592163086 \t train loss 0.1325761079788208\n",
      "Model 11 0.959\n",
      "Epoch :  1 \t test loss : 3.058635711669922 \t train loss 2.9834115505218506\n",
      "Epoch :  101 \t test loss : 0.393554151058197 \t train loss 0.11579425632953644\n",
      "Model 12 0.958\n",
      "Epoch :  1 \t test loss : 2.2853598594665527 \t train loss 2.278837203979492\n",
      "Epoch :  101 \t test loss : 0.5501793622970581 \t train loss 0.24758785963058472\n",
      "Model 13 0.9359999999999999\n",
      "Epoch :  1 \t test loss : 2.2131307125091553 \t train loss 2.2031521797180176\n",
      "Epoch :  101 \t test loss : 0.4466537535190582 \t train loss 0.13332918286323547\n",
      "Model 14 0.962\n",
      "Epoch :  1 \t test loss : 2.085878849029541 \t train loss 2.062612533569336\n",
      "Epoch :  101 \t test loss : 0.35931846499443054 \t train loss 0.06972405314445496\n",
      "Model 15 0.972\n",
      "Epoch :  1 \t test loss : 1.7612361907958984 \t train loss 1.7603822946548462\n",
      "Epoch :  101 \t test loss : 0.33371788263320923 \t train loss 0.049295853823423386\n",
      "Model 16 0.972\n",
      "Epoch :  1 \t test loss : 2.6702053546905518 \t train loss 2.6611316204071045\n",
      "Epoch :  101 \t test loss : 1.2611171007156372 \t train loss 1.165686011314392\n",
      "Model 0 0.7969999999999999\n",
      "Epoch :  1 \t test loss : 2.241076946258545 \t train loss 2.249091386795044\n",
      "Epoch :  101 \t test loss : 0.9719281196594238 \t train loss 0.9126628637313843\n",
      "Model 1 0.858\n",
      "Epoch :  1 \t test loss : 2.793327808380127 \t train loss 2.770610809326172\n",
      "Epoch :  101 \t test loss : 0.9792555570602417 \t train loss 0.8757506012916565\n",
      "Model 2 0.842\n",
      "Epoch :  1 \t test loss : 4.255376815795898 \t train loss 4.217233180999756\n",
      "Epoch :  101 \t test loss : 1.036844253540039 \t train loss 0.8649070858955383\n",
      "Model 3 0.854\n",
      "Epoch :  1 \t test loss : 3.6747632026672363 \t train loss 3.559532403945923\n",
      "Epoch :  101 \t test loss : 0.862497091293335 \t train loss 0.6659574508666992\n",
      "Model 4 0.865\n",
      "Epoch :  1 \t test loss : 2.3990375995635986 \t train loss 2.366863965988159\n",
      "Epoch :  101 \t test loss : 0.67896568775177 \t train loss 0.536878764629364\n",
      "Model 5 0.901\n",
      "Epoch :  1 \t test loss : 2.262723207473755 \t train loss 2.23785662651062\n",
      "Epoch :  101 \t test loss : 0.656379759311676 \t train loss 0.429487019777298\n",
      "Model 6 0.902\n",
      "Epoch :  1 \t test loss : 2.3049476146698 \t train loss 2.2843525409698486\n",
      "Epoch :  101 \t test loss : 0.552128255367279 \t train loss 0.344487726688385\n",
      "Model 7 0.919\n",
      "Epoch :  1 \t test loss : 2.3716793060302734 \t train loss 2.3582847118377686\n",
      "Epoch :  101 \t test loss : 0.5026686787605286 \t train loss 0.25846999883651733\n",
      "Model 8 0.926\n",
      "Epoch :  1 \t test loss : 2.2239935398101807 \t train loss 2.20868182182312\n",
      "Epoch :  101 \t test loss : 0.4792274534702301 \t train loss 0.28582441806793213\n",
      "Model 9 0.9319999999999999\n",
      "Epoch :  1 \t test loss : 2.201247453689575 \t train loss 2.229081392288208\n",
      "Epoch :  101 \t test loss : 0.44985103607177734 \t train loss 0.2111438363790512\n",
      "Model 10 0.9390000000000001\n",
      "Epoch :  1 \t test loss : 2.2090249061584473 \t train loss 2.1727421283721924\n",
      "Epoch :  101 \t test loss : 0.4028697609901428 \t train loss 0.14045028388500214\n",
      "Model 11 0.954\n",
      "Epoch :  1 \t test loss : 1.9581891298294067 \t train loss 1.9568843841552734\n",
      "Epoch :  101 \t test loss : 0.4326448440551758 \t train loss 0.1053176000714302\n",
      "Model 12 0.957\n",
      "Epoch :  1 \t test loss : 2.28033709526062 \t train loss 2.2491655349731445\n",
      "Epoch :  101 \t test loss : 0.4961765706539154 \t train loss 0.2356313318014145\n",
      "Model 13 0.9339999999999999\n",
      "Epoch :  1 \t test loss : 2.2198657989501953 \t train loss 2.20969557762146\n",
      "Epoch :  101 \t test loss : 0.42142269015312195 \t train loss 0.1199120357632637\n",
      "Model 14 0.96\n",
      "Epoch :  1 \t test loss : 2.3027970790863037 \t train loss 2.29972767829895\n",
      "Epoch :  101 \t test loss : 0.37395283579826355 \t train loss 0.08728467673063278\n",
      "Model 15 0.966\n",
      "Epoch :  1 \t test loss : 2.0788955688476562 \t train loss 2.070614814758301\n",
      "Epoch :  101 \t test loss : 0.3788384795188904 \t train loss 0.046461302787065506\n",
      "Model 16 0.961\n",
      "Epoch :  1 \t test loss : 5.6086931228637695 \t train loss 5.611189365386963\n",
      "Epoch :  101 \t test loss : 1.5024131536483765 \t train loss 1.4598857164382935\n",
      "Model 0 0.78\n",
      "Epoch :  1 \t test loss : 2.9495773315429688 \t train loss 3.095189094543457\n",
      "Epoch :  101 \t test loss : 1.3026883602142334 \t train loss 1.2176852226257324\n",
      "Model 1 0.836\n",
      "Epoch :  1 \t test loss : 3.803191900253296 \t train loss 3.8022942543029785\n",
      "Epoch :  101 \t test loss : 1.070581078529358 \t train loss 1.0106202363967896\n",
      "Model 2 0.825\n",
      "Epoch :  1 \t test loss : 3.0380547046661377 \t train loss 2.9977047443389893\n",
      "Epoch :  101 \t test loss : 0.9515389800071716 \t train loss 0.9158317446708679\n",
      "Model 3 0.847\n",
      "Epoch :  1 \t test loss : 2.9041037559509277 \t train loss 2.8779492378234863\n",
      "Epoch :  101 \t test loss : 0.8061070442199707 \t train loss 0.6748722195625305\n",
      "Model 4 0.871\n",
      "Epoch :  1 \t test loss : 2.7522408962249756 \t train loss 2.8437235355377197\n",
      "Epoch :  101 \t test loss : 1.0541906356811523 \t train loss 0.9792546629905701\n",
      "Model 5 0.884\n",
      "Epoch :  1 \t test loss : 2.837517499923706 \t train loss 2.8551926612854004\n",
      "Epoch :  101 \t test loss : 0.7148707509040833 \t train loss 0.6096847653388977\n",
      "Model 6 0.916\n",
      "Epoch :  1 \t test loss : 2.567314386367798 \t train loss 2.6816823482513428\n",
      "Epoch :  101 \t test loss : 0.6549990773200989 \t train loss 0.5547705888748169\n",
      "Model 7 0.913\n",
      "Epoch :  1 \t test loss : 3.2919697761535645 \t train loss 3.4581027030944824\n",
      "Epoch :  101 \t test loss : 0.6368154287338257 \t train loss 0.5782509446144104\n",
      "Model 8 0.919\n",
      "Epoch :  1 \t test loss : 2.373575448989868 \t train loss 2.3635642528533936\n",
      "Epoch :  101 \t test loss : 0.4803756773471832 \t train loss 0.3394645154476166\n",
      "Model 9 0.94\n",
      "Epoch :  1 \t test loss : 2.2421674728393555 \t train loss 2.243879795074463\n",
      "Epoch :  101 \t test loss : 0.4193026125431061 \t train loss 0.2695658504962921\n",
      "Model 10 0.94\n",
      "Epoch :  1 \t test loss : 2.2476961612701416 \t train loss 2.2320125102996826\n",
      "Epoch :  101 \t test loss : 0.41043511033058167 \t train loss 0.22095276415348053\n",
      "Model 11 0.955\n",
      "Epoch :  1 \t test loss : 2.2743451595306396 \t train loss 2.2976975440979004\n",
      "Epoch :  101 \t test loss : 0.4400799572467804 \t train loss 0.20068763196468353\n",
      "Model 12 0.949\n",
      "Epoch :  1 \t test loss : 2.2415802478790283 \t train loss 2.209980010986328\n",
      "Epoch :  101 \t test loss : 0.47713103890419006 \t train loss 0.2549971342086792\n",
      "Model 13 0.95\n",
      "Epoch :  1 \t test loss : 2.236358642578125 \t train loss 2.2416741847991943\n",
      "Epoch :  101 \t test loss : 0.3699198365211487 \t train loss 0.15140339732170105\n",
      "Model 14 0.961\n",
      "Epoch :  1 \t test loss : 2.2082409858703613 \t train loss 2.186126708984375\n",
      "Epoch :  101 \t test loss : 0.36185160279273987 \t train loss 0.08085287362337112\n",
      "Model 15 0.964\n",
      "Epoch :  1 \t test loss : 2.1161983013153076 \t train loss 2.1204378604888916\n",
      "Epoch :  101 \t test loss : 0.38734668493270874 \t train loss 0.061033014208078384\n",
      "Model 16 0.966\n",
      "Epoch :  1 \t test loss : 5.203423976898193 \t train loss 5.010680675506592\n",
      "Epoch :  101 \t test loss : 1.4197289943695068 \t train loss 1.4613525867462158\n",
      "Model 0 0.785\n",
      "Epoch :  1 \t test loss : 2.9226162433624268 \t train loss 2.90887451171875\n",
      "Epoch :  101 \t test loss : 1.0558098554611206 \t train loss 1.0117031335830688\n",
      "Model 1 0.84\n",
      "Epoch :  1 \t test loss : 3.1354458332061768 \t train loss 3.119170904159546\n",
      "Epoch :  101 \t test loss : 0.9415076971054077 \t train loss 0.9189735054969788\n",
      "Model 2 0.866\n",
      "Epoch :  1 \t test loss : 3.6462862491607666 \t train loss 3.7042787075042725\n",
      "Epoch :  101 \t test loss : 0.990926206111908 \t train loss 1.0034806728363037\n",
      "Model 3 0.862\n",
      "Epoch :  1 \t test loss : 15.746834754943848 \t train loss 15.937115669250488\n",
      "Epoch :  101 \t test loss : 1.3974910974502563 \t train loss 1.3963762521743774\n",
      "Model 4 0.843\n",
      "Epoch :  1 \t test loss : 2.4860384464263916 \t train loss 2.4929096698760986\n",
      "Epoch :  101 \t test loss : 0.7105883359909058 \t train loss 0.6138085126876831\n",
      "Model 5 0.904\n",
      "Epoch :  1 \t test loss : 3.512631893157959 \t train loss 3.5533502101898193\n",
      "Epoch :  101 \t test loss : 0.711517333984375 \t train loss 0.7031568288803101\n",
      "Model 6 0.905\n",
      "Epoch :  1 \t test loss : 2.4726686477661133 \t train loss 2.4404749870300293\n",
      "Epoch :  101 \t test loss : 0.5286508798599243 \t train loss 0.4543987512588501\n",
      "Model 7 0.933\n",
      "Epoch :  1 \t test loss : 2.7471868991851807 \t train loss 2.795107364654541\n",
      "Epoch :  101 \t test loss : 0.6412293314933777 \t train loss 0.5882951021194458\n",
      "Model 8 0.931\n",
      "Epoch :  1 \t test loss : 2.3727498054504395 \t train loss 2.389622211456299\n",
      "Epoch :  101 \t test loss : 0.4826273024082184 \t train loss 0.3974073529243469\n",
      "Model 9 0.9390000000000001\n",
      "Epoch :  1 \t test loss : 2.3275017738342285 \t train loss 2.3035149574279785\n",
      "Epoch :  101 \t test loss : 0.4098125398159027 \t train loss 0.2752474248409271\n",
      "Model 10 0.949\n",
      "Epoch :  1 \t test loss : 2.300995349884033 \t train loss 2.2817392349243164\n",
      "Epoch :  101 \t test loss : 0.4220057427883148 \t train loss 0.25831839442253113\n",
      "Model 11 0.95\n",
      "Epoch :  1 \t test loss : 2.2768566608428955 \t train loss 2.3221802711486816\n",
      "Epoch :  101 \t test loss : 0.35353565216064453 \t train loss 0.16716255247592926\n",
      "Model 12 0.964\n",
      "Epoch :  1 \t test loss : 2.296499252319336 \t train loss 2.2742421627044678\n",
      "Epoch :  101 \t test loss : 0.3940722346305847 \t train loss 0.25436875224113464\n",
      "Model 13 0.951\n",
      "Epoch :  1 \t test loss : 2.3014867305755615 \t train loss 2.2888758182525635\n",
      "Epoch :  101 \t test loss : 0.37844955921173096 \t train loss 0.1451863795518875\n",
      "Model 14 0.967\n",
      "Epoch :  1 \t test loss : 2.2820329666137695 \t train loss 2.3102457523345947\n",
      "Epoch :  101 \t test loss : 0.3671451807022095 \t train loss 0.09394827485084534\n",
      "Model 15 0.968\n",
      "Epoch :  1 \t test loss : 2.2171952724456787 \t train loss 2.257277011871338\n",
      "Epoch :  101 \t test loss : 0.36399027705192566 \t train loss 0.08028437942266464\n",
      "Model 16 0.978\n",
      "It took 11.737685736020406 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "epochs = 200\n",
    "accuracies = torch.empty(17, 4, dtype=torch.float)\n",
    "\n",
    "for i in range(4):\n",
    "    train_input, train_target, train_classes, test_input, test_target, test_classes = prolog.generate_pair_sets(1000)\n",
    "\n",
    "    train_input = train_input.view(-1, 14, 14).unsqueeze(1).cuda()\n",
    "    test_input = test_input.view(-1, 14, 14).unsqueeze(1).cuda()\n",
    "    train_classes = train_classes.view(2000).cuda()\n",
    "    test_classes = test_classes.view(2000).cuda()\n",
    "    test_target = test_target.cuda()\n",
    "\n",
    "    for j in range(17):\n",
    "        _, _, _, best_accuracy = train_model_10(models[j], train_input, train_classes, test_input, test_target, test_classes,\\\n",
    "                                              epochs=epochs, lr = 0.08)\n",
    "        print('Model', j , best_accuracy)\n",
    "        accuracies[j][i] = best_accuracy\n",
    "\n",
    "\n",
    "minute = (time.time()-start) / 60\n",
    "print('It took', minute, 'minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "EiJyfzQ556kp",
    "outputId": "ce9ff6f1-3751-48af-9c8e-b3c727e7d445"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7922, 0.8345, 0.8470, 0.8592, 0.8582, 0.8947, 0.9100, 0.9223, 0.9280,\n",
       "        0.9375, 0.9442, 0.9545, 0.9570, 0.9427, 0.9625, 0.9675, 0.9693])"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ercrVkip56o2",
    "outputId": "9c0d6769-c8a8-475a-a9ab-d67b7e8d4b76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0121, 0.0225, 0.0176, 0.0116, 0.0124, 0.0094, 0.0076, 0.0085, 0.0073,\n",
       "        0.0037, 0.0055, 0.0037, 0.0062, 0.0090, 0.0031, 0.0034, 0.0074])"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.std(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZppUsQWLopY"
   },
   "source": [
    "Once again model with more parameter seems to show better result, so let's try the 10-dim approch with bigConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5-Cn41U56rY"
   },
   "outputs": [],
   "source": [
    "class bigConvNet_1(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(bigConvNet_1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=2, stride = 1)\n",
    "        self.fc1 = nn.Linear(4*128, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 4*128)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class bigConvNet_2(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(bigConvNet_2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(64, 256, kernel_size=2, stride = 1)\n",
    "        self.fc1 = nn.Linear(4*256, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 4*256)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class bigConvNet_3(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(bigConvNet_3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 100, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(100, 200, kernel_size=2, stride = 1)\n",
    "        self.fc1 = nn.Linear(4*200, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 4*200)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class bigConvNet_4(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(bigConvNet_4, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 128, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(128, 1024, kernel_size=2, stride = 1)\n",
    "        self.fc1 = nn.Linear(4*1024, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 4*1024)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "TyrotGx856mv",
    "outputId": "f67f5139-82bf-4e16-a505-2f972f26d62f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2316 59376 117872 121260 731312\n",
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "baseline = BaseLine()\n",
    "\n",
    "model_1_0 = bigConvNet_1(50)\n",
    "model_1_1 = bigConvNet_1(100)\n",
    "model_1_2 = bigConvNet_1(500)\n",
    "model_1_3 = bigConvNet_1(1000)\n",
    "\n",
    "\n",
    "model_2_0 = bigConvNet_2(50)\n",
    "model_2_1 = bigConvNet_2(100)\n",
    "model_2_2 = bigConvNet_2(500)\n",
    "model_2_3 = bigConvNet_2(1000)\n",
    "\n",
    "\n",
    "model_3_0 = bigConvNet_3(50)\n",
    "model_3_1 = bigConvNet_3(100)\n",
    "model_3_2 = bigConvNet_3(500)\n",
    "model_3_3 = bigConvNet_3(1000)\n",
    "\n",
    "\n",
    "\n",
    "model_4_0 = bigConvNet_4(50)\n",
    "model_4_1 = bigConvNet_4(100)\n",
    "model_4_2 = bigConvNet_4(500)\n",
    "model_4_3 = bigConvNet_4(1000)\n",
    "\n",
    "\n",
    "\n",
    "print(count_parameters(baseline),\n",
    "count_parameters(model_1_0),\n",
    "count_parameters(model_2_0),\n",
    "count_parameters(model_3_0),\n",
    "count_parameters(model_4_0))\n",
    "\n",
    "models = [baseline, model_1_0,model_1_1, model_1_2, model_1_3,\n",
    "          model_2_0,model_2_1, model_2_2, model_2_3,\n",
    "          model_3_0,model_3_1, model_3_2, model_3_3,\n",
    "          model_4_0,model_4_1, model_4_2, model_4_3]\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\") \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "\n",
    "for model in models:\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CCsKFO4WMJ4g",
    "outputId": "1c9e4b06-e411-413b-bfd4-69ddad255b8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 \t test loss : 8.584234237670898 \t train loss 8.6730318069458\n",
      "Epoch :  101 \t test loss : 1.4146349430084229 \t train loss 1.37984299659729\n",
      "Model 0 0.804\n",
      "Epoch :  1 \t test loss : 2.3089544773101807 \t train loss 2.3066036701202393\n",
      "Epoch :  101 \t test loss : 0.5827301740646362 \t train loss 0.34849101305007935\n",
      "Model 1 0.9410000000000001\n",
      "Epoch :  1 \t test loss : 2.198092460632324 \t train loss 2.1921257972717285\n",
      "Epoch :  101 \t test loss : 0.3984755277633667 \t train loss 0.09769120812416077\n",
      "Model 2 0.964\n",
      "Epoch :  1 \t test loss : 1.6123627424240112 \t train loss 1.6273112297058105\n",
      "Epoch :  101 \t test loss : 0.32927724719047546 \t train loss 0.036315180361270905\n",
      "Model 3 0.966\n",
      "Epoch :  1 \t test loss : 1.6482770442962646 \t train loss 1.7160600423812866\n",
      "Epoch :  101 \t test loss : 0.33036988973617554 \t train loss 0.03444743528962135\n",
      "Model 4 0.965\n",
      "Epoch :  1 \t test loss : 2.307478189468384 \t train loss 2.3044278621673584\n",
      "Epoch :  101 \t test loss : 0.8569839000701904 \t train loss 0.7308889031410217\n",
      "Model 5 0.907\n",
      "Epoch :  1 \t test loss : 2.1346473693847656 \t train loss 2.126518487930298\n",
      "Epoch :  101 \t test loss : 0.40520718693733215 \t train loss 0.05927945300936699\n",
      "Model 6 0.96\n",
      "Epoch :  1 \t test loss : 1.8111971616744995 \t train loss 1.8376870155334473\n",
      "Epoch :  101 \t test loss : 0.35322868824005127 \t train loss 0.048440705984830856\n",
      "Model 7 0.968\n",
      "Epoch :  1 \t test loss : 1.6539162397384644 \t train loss 1.6580159664154053\n",
      "Epoch :  101 \t test loss : 0.358074426651001 \t train loss 0.026282448321580887\n",
      "Model 8 0.97\n",
      "Epoch :  1 \t test loss : 2.3056414127349854 \t train loss 2.303722858428955\n",
      "Epoch :  101 \t test loss : 0.5786694288253784 \t train loss 0.2733435332775116\n",
      "Model 9 0.937\n",
      "Epoch :  1 \t test loss : 2.114353656768799 \t train loss 2.1047332286834717\n",
      "Epoch :  101 \t test loss : 0.4109850525856018 \t train loss 0.07559816539287567\n",
      "Model 10 0.962\n",
      "Epoch :  1 \t test loss : 1.6390018463134766 \t train loss 1.6862560510635376\n",
      "Epoch :  101 \t test loss : 0.33341431617736816 \t train loss 0.021837078034877777\n",
      "Model 11 0.973\n",
      "Epoch :  1 \t test loss : 1.7063778638839722 \t train loss 1.7351652383804321\n",
      "Epoch :  101 \t test loss : 0.4014225900173187 \t train loss 0.04094744101166725\n",
      "Model 12 0.968\n",
      "Epoch :  1 \t test loss : 2.303102731704712 \t train loss 2.3023948669433594\n",
      "Epoch :  101 \t test loss : 2.304971694946289 \t train loss 2.3012609481811523\n",
      "Model 13 0.575\n",
      "Epoch :  1 \t test loss : 2.2149288654327393 \t train loss 2.247647523880005\n",
      "Epoch :  101 \t test loss : 0.9511603713035583 \t train loss 0.7009263038635254\n",
      "Model 14 0.897\n",
      "Epoch :  1 \t test loss : 1.3398343324661255 \t train loss 1.3750951290130615\n",
      "Epoch :  101 \t test loss : 0.4039670526981354 \t train loss 0.04519331082701683\n",
      "Model 15 0.972\n",
      "Epoch :  1 \t test loss : 1.5103421211242676 \t train loss 1.5359768867492676\n",
      "Epoch :  101 \t test loss : 0.47825175523757935 \t train loss 0.06808781623840332\n",
      "Model 16 0.964\n",
      "Epoch :  1 \t test loss : 3.1085100173950195 \t train loss 2.953713893890381\n",
      "Epoch :  101 \t test loss : 1.3162287473678589 \t train loss 1.2755954265594482\n",
      "Model 0 0.8029999999999999\n",
      "Epoch :  1 \t test loss : 2.3348562717437744 \t train loss 2.3354151248931885\n",
      "Epoch :  101 \t test loss : 0.4746004045009613 \t train loss 0.19371677935123444\n",
      "Model 1 0.935\n",
      "Epoch :  1 \t test loss : 2.2330572605133057 \t train loss 2.2397706508636475\n",
      "Epoch :  101 \t test loss : 0.4140118658542633 \t train loss 0.10503435879945755\n",
      "Model 2 0.955\n",
      "Epoch :  1 \t test loss : 2.1045849323272705 \t train loss 2.1871745586395264\n",
      "Epoch :  101 \t test loss : 0.4033568203449249 \t train loss 0.05234762653708458\n",
      "Model 3 0.963\n",
      "Epoch :  1 \t test loss : 2.0565173625946045 \t train loss 2.0738494396209717\n",
      "Epoch :  101 \t test loss : 0.3670274019241333 \t train loss 0.03919148072600365\n",
      "Model 4 0.962\n",
      "Epoch :  1 \t test loss : 2.366824150085449 \t train loss 2.369853973388672\n",
      "Epoch :  101 \t test loss : 0.5103574395179749 \t train loss 0.26365557312965393\n",
      "Model 5 0.921\n",
      "Epoch :  1 \t test loss : 2.2860329151153564 \t train loss 2.259031057357788\n",
      "Epoch :  101 \t test loss : 0.3882059156894684 \t train loss 0.07475019991397858\n",
      "Model 6 0.958\n",
      "Epoch :  1 \t test loss : 2.060981273651123 \t train loss 2.117658853530884\n",
      "Epoch :  101 \t test loss : 0.43916043639183044 \t train loss 0.047203805297613144\n",
      "Model 7 0.961\n",
      "Epoch :  1 \t test loss : 2.095884323120117 \t train loss 2.0644876956939697\n",
      "Epoch :  101 \t test loss : 0.3513663411140442 \t train loss 0.026469925418496132\n",
      "Model 8 0.962\n",
      "Epoch :  1 \t test loss : 2.3874123096466064 \t train loss 2.3873887062072754\n",
      "Epoch :  101 \t test loss : 0.4376682639122009 \t train loss 0.13706786930561066\n",
      "Model 9 0.9410000000000001\n",
      "Epoch :  1 \t test loss : 2.164158582687378 \t train loss 2.169013738632202\n",
      "Epoch :  101 \t test loss : 0.407600998878479 \t train loss 0.07594373822212219\n",
      "Model 10 0.96\n",
      "Epoch :  1 \t test loss : 2.085761070251465 \t train loss 2.10575270652771\n",
      "Epoch :  101 \t test loss : 0.3680609166622162 \t train loss 0.04147028550505638\n",
      "Model 11 0.966\n",
      "Epoch :  1 \t test loss : 2.0066323280334473 \t train loss 2.0122854709625244\n",
      "Epoch :  101 \t test loss : 0.46382343769073486 \t train loss 0.04678531736135483\n",
      "Model 12 0.965\n",
      "Epoch :  1 \t test loss : 2.3023600578308105 \t train loss 2.3041505813598633\n",
      "Epoch :  101 \t test loss : 2.3026249408721924 \t train loss 2.3002805709838867\n",
      "Model 13 0.5640000000000001\n",
      "Epoch :  1 \t test loss : 2.52771258354187 \t train loss 2.516197443008423\n",
      "Epoch :  101 \t test loss : 1.7691130638122559 \t train loss 1.6526179313659668\n",
      "Model 14 0.74\n",
      "Epoch :  1 \t test loss : 2.0530354976654053 \t train loss 2.109416961669922\n",
      "Epoch :  101 \t test loss : 0.4798164963722229 \t train loss 0.02833906002342701\n",
      "Model 15 0.964\n",
      "Epoch :  1 \t test loss : 2.1178743839263916 \t train loss 2.137376070022583\n",
      "Epoch :  101 \t test loss : 0.42956602573394775 \t train loss 0.04935184493660927\n",
      "Model 16 0.962\n",
      "Epoch :  1 \t test loss : 5.378194808959961 \t train loss 5.6159234046936035\n",
      "Epoch :  101 \t test loss : 1.3368117809295654 \t train loss 1.326062798500061\n",
      "Model 0 0.776\n",
      "Epoch :  1 \t test loss : 2.3607866764068604 \t train loss 2.3812499046325684\n",
      "Epoch :  101 \t test loss : 0.41232118010520935 \t train loss 0.16161277890205383\n",
      "Model 1 0.9390000000000001\n",
      "Epoch :  1 \t test loss : 2.2652575969696045 \t train loss 2.299644708633423\n",
      "Epoch :  101 \t test loss : 0.3040594756603241 \t train loss 0.08670699596405029\n",
      "Model 2 0.969\n",
      "Epoch :  1 \t test loss : 2.053574800491333 \t train loss 2.0447964668273926\n",
      "Epoch :  101 \t test loss : 0.312605082988739 \t train loss 0.04146498814225197\n",
      "Model 3 0.967\n",
      "Epoch :  1 \t test loss : 2.1997745037078857 \t train loss 2.2069382667541504\n",
      "Epoch :  101 \t test loss : 0.37719812989234924 \t train loss 0.03992636874318123\n",
      "Model 4 0.968\n",
      "Epoch :  1 \t test loss : 2.4294075965881348 \t train loss 2.465510606765747\n",
      "Epoch :  101 \t test loss : 0.4550841152667999 \t train loss 0.19697582721710205\n",
      "Model 5 0.9359999999999999\n",
      "Epoch :  1 \t test loss : 2.2100026607513428 \t train loss 2.262938976287842\n",
      "Epoch :  101 \t test loss : 0.3199860453605652 \t train loss 0.07088412344455719\n",
      "Model 6 0.964\n",
      "Epoch :  1 \t test loss : 2.1735692024230957 \t train loss 2.1665611267089844\n",
      "Epoch :  101 \t test loss : 0.3021424114704132 \t train loss 0.02747969888150692\n",
      "Model 7 0.966\n",
      "Epoch :  1 \t test loss : 2.107220411300659 \t train loss 2.141728401184082\n",
      "Epoch :  101 \t test loss : 0.29836493730545044 \t train loss 0.031179506331682205\n",
      "Model 8 0.962\n",
      "Epoch :  1 \t test loss : 2.4202613830566406 \t train loss 2.444944381713867\n",
      "Epoch :  101 \t test loss : 0.455289751291275 \t train loss 0.13341765105724335\n",
      "Model 9 0.95\n",
      "Epoch :  1 \t test loss : 2.2615299224853516 \t train loss 2.3623039722442627\n",
      "Epoch :  101 \t test loss : 0.3743501305580139 \t train loss 0.06662797182798386\n",
      "Model 10 0.962\n",
      "Epoch :  1 \t test loss : 2.246589183807373 \t train loss 2.2401411533355713\n",
      "Epoch :  101 \t test loss : 0.3243280351161957 \t train loss 0.04154874384403229\n",
      "Model 11 0.968\n",
      "Epoch :  1 \t test loss : 2.133251190185547 \t train loss 2.1550958156585693\n",
      "Epoch :  101 \t test loss : 0.3719547390937805 \t train loss 0.027525989338755608\n",
      "Model 12 0.967\n",
      "Epoch :  1 \t test loss : 2.3014326095581055 \t train loss 2.2989206314086914\n",
      "Epoch :  101 \t test loss : 2.3004398345947266 \t train loss 2.2963600158691406\n",
      "Model 13 0.54\n",
      "Epoch :  1 \t test loss : 2.746966600418091 \t train loss 2.7688040733337402\n",
      "Epoch :  101 \t test loss : 2.306642532348633 \t train loss 2.303328037261963\n",
      "Model 14 0.54\n",
      "Epoch :  1 \t test loss : 2.2781851291656494 \t train loss 2.2852320671081543\n",
      "Epoch :  101 \t test loss : 0.3285822570323944 \t train loss 0.08056065440177917\n",
      "Model 15 0.968\n",
      "Epoch :  1 \t test loss : 2.279984712600708 \t train loss 2.254749059677124\n",
      "Epoch :  101 \t test loss : 0.4494147002696991 \t train loss 0.055192168802022934\n",
      "Model 16 0.967\n",
      "Epoch :  1 \t test loss : 2.834219455718994 \t train loss 2.9551656246185303\n",
      "Epoch :  101 \t test loss : 1.2072958946228027 \t train loss 1.1579053401947021\n",
      "Model 0 0.8\n",
      "Epoch :  1 \t test loss : 2.2643072605133057 \t train loss 2.296283721923828\n",
      "Epoch :  101 \t test loss : 0.39712339639663696 \t train loss 0.18051758408546448\n",
      "Model 1 0.947\n",
      "Epoch :  1 \t test loss : 2.1982157230377197 \t train loss 2.2250559329986572\n",
      "Epoch :  101 \t test loss : 0.33141374588012695 \t train loss 0.10291602462530136\n",
      "Model 2 0.966\n",
      "Epoch :  1 \t test loss : 2.1709930896759033 \t train loss 2.1820099353790283\n",
      "Epoch :  101 \t test loss : 0.43203049898147583 \t train loss 0.04894621670246124\n",
      "Model 3 0.969\n",
      "Epoch :  1 \t test loss : 2.2029764652252197 \t train loss 2.190187931060791\n",
      "Epoch :  101 \t test loss : 0.3504871428012848 \t train loss 0.07732093334197998\n",
      "Model 4 0.974\n",
      "Epoch :  1 \t test loss : 2.531977653503418 \t train loss 2.512531280517578\n",
      "Epoch :  101 \t test loss : 0.45187047123908997 \t train loss 0.21039240062236786\n",
      "Model 5 0.94\n",
      "Epoch :  1 \t test loss : 2.357464075088501 \t train loss 2.3642618656158447\n",
      "Epoch :  101 \t test loss : 0.39636239409446716 \t train loss 0.06584661453962326\n",
      "Model 6 0.96\n",
      "Epoch :  1 \t test loss : 2.20864200592041 \t train loss 2.236860990524292\n",
      "Epoch :  101 \t test loss : 0.3587549328804016 \t train loss 0.07365459203720093\n",
      "Model 7 0.97\n",
      "Epoch :  1 \t test loss : 2.1933212280273438 \t train loss 2.1651370525360107\n",
      "Epoch :  101 \t test loss : 0.3889879286289215 \t train loss 0.046437036246061325\n",
      "Model 8 0.974\n",
      "Epoch :  1 \t test loss : 2.4060275554656982 \t train loss 2.388964891433716\n",
      "Epoch :  101 \t test loss : 0.37214815616607666 \t train loss 0.15988789498806\n",
      "Model 9 0.949\n",
      "Epoch :  1 \t test loss : 2.3419511318206787 \t train loss 2.354809284210205\n",
      "Epoch :  101 \t test loss : 0.4232492446899414 \t train loss 0.07242514193058014\n",
      "Model 10 0.968\n",
      "Epoch :  1 \t test loss : 2.230576515197754 \t train loss 2.2900354862213135\n",
      "Epoch :  101 \t test loss : 0.3364090621471405 \t train loss 0.06124977394938469\n",
      "Model 11 0.968\n",
      "Epoch :  1 \t test loss : 2.1201770305633545 \t train loss 2.1548044681549072\n",
      "Epoch :  101 \t test loss : 0.4650097191333771 \t train loss 0.062115080654621124\n",
      "Model 12 0.97\n",
      "Epoch :  1 \t test loss : 2.3048505783081055 \t train loss 2.3040378093719482\n",
      "Epoch :  101 \t test loss : 2.304598569869995 \t train loss 2.3001248836517334\n",
      "Model 13 0.538\n",
      "Epoch :  1 \t test loss : 2.3048059940338135 \t train loss 2.304056167602539\n",
      "Epoch :  101 \t test loss : 2.304598331451416 \t train loss 2.3001246452331543\n",
      "Model 14 0.538\n",
      "Epoch :  1 \t test loss : 2.1435248851776123 \t train loss 2.193681240081787\n",
      "Epoch :  101 \t test loss : 0.4372870624065399 \t train loss 0.027661841362714767\n",
      "Model 15 0.97\n",
      "Epoch :  1 \t test loss : 2.17578387260437 \t train loss 2.2011160850524902\n",
      "Epoch :  101 \t test loss : 0.4421676695346832 \t train loss 0.019100086763501167\n",
      "Model 16 0.971\n",
      "It took 23.715359632174174 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "epochs = 200\n",
    "accuracies = torch.empty(17, 4, dtype=torch.float)\n",
    "\n",
    "for i in range(4):\n",
    "    train_input, train_target, train_classes, test_input, test_target, test_classes = prolog.generate_pair_sets(1000)\n",
    "\n",
    "    train_input = train_input.view(-1, 14, 14).unsqueeze(1).cuda()\n",
    "    test_input = test_input.view(-1, 14, 14).unsqueeze(1).cuda()\n",
    "    train_classes = train_classes.view(2000).cuda()\n",
    "    test_classes = test_classes.view(2000).cuda()\n",
    "    test_target = test_target.cuda()\n",
    "\n",
    "    for j in range(17):\n",
    "        _, _, _, best_accuracy = train_model_10(models[j], train_input, train_classes, test_input, test_target, test_classes,\\\n",
    "                                              epochs=epochs, lr = 0.08)\n",
    "        print('Model', j , best_accuracy)\n",
    "        accuracies[j][i] = best_accuracy\n",
    "\n",
    "\n",
    "minute = (time.time()-start) / 60\n",
    "print('It took', minute, 'minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "c2KtDq9JMJ6o",
    "outputId": "103484f3-574c-4354-b3af-f22213b073bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7957, 0.9405, 0.9635, 0.9662, 0.9672, 0.9260, 0.9605, 0.9663, 0.9670,\n",
       "        0.9442, 0.9630, 0.9688, 0.9675, 0.5543, 0.6788, 0.9685, 0.9660])"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "xtipaLCqMlxZ",
    "outputId": "bd900212-6309-424a-de2f-b67013549adf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0133, 0.0050, 0.0060, 0.0025, 0.0051, 0.0151, 0.0025, 0.0039, 0.0060,\n",
       "        0.0063, 0.0035, 0.0030, 0.0021, 0.0182, 0.1736, 0.0034, 0.0039])"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.std(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PnP07CXpMpny"
   },
   "source": [
    "It looks like bigConvNet1 show the better result without increasing too much the number of parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "koNSHRcpMlz9"
   },
   "source": [
    "Trying the best models with various dropout values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2YiOiR1rTFxv"
   },
   "outputs": [],
   "source": [
    "class ConvNet_4(nn.Module):\n",
    "    def __init__(self, nb_hidden, dp1, dp2):\n",
    "        super(ConvNet_4, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(32, 128, kernel_size=2, stride = 1)\n",
    "        self.fc1 = nn.Linear(512, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "        self.dropout1 = nn.Dropout2d(dp1)\n",
    "        self.dropout2 = nn.Dropout2d(dp2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 512)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class bigConvNet_3(nn.Module):\n",
    "    def __init__(self, nb_hidden, dp1, dp2):\n",
    "        super(bigConvNet_3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 100, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(100, 200, kernel_size=2, stride = 1)\n",
    "        self.fc1 = nn.Linear(4*200, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "        self.dropout1 = nn.Dropout2d(dp1)\n",
    "        self.dropout2 = nn.Dropout2d(dp2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 4*200)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ConvNet_4(1000, 0.0, 0.0)\n",
    "model2 = ConvNet_4(1000, 0.5, 0.0)\n",
    "model3 = ConvNet_4(1000, 0.0, 0.5)\n",
    "model4 = ConvNet_4(1000, 0.5, 0.5)\n",
    "model5 = bigConvNet_3(500, 0.0, 0.0)\n",
    "model6 = bigConvNet_3(500, 0.5, 0.0)\n",
    "model7 = bigConvNet_3(500, 0.0, 0.5)\n",
    "model8 = bigConvNet_3(500, 0.5, 0.5)\n",
    "\n",
    "models = [model1, model2, model3, model4, model5, model6, model7, model8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = torch.empty(8, 10, dtype=torch.float)\n",
    "\n",
    "for i in range(10):\n",
    "    train_input, train_target, train_classes, test_input, test_target, test_classes = prolog.generate_pair_sets(1000)\n",
    "\n",
    "    train_input = train_input.view(-1, 14, 14).unsqueeze(1)\n",
    "    test_input = test_input.view(-1, 14, 14).unsqueeze(1)\n",
    "    train_classes = train_classes.view(2000)\n",
    "    test_classes = test_classes.view(2000)\n",
    "    test_target = test_target\n",
    "\n",
    "    for j in range(8):\n",
    "        if i > 3:\n",
    "            epochs = 200\n",
    "        else:\n",
    "            epochs = 100\n",
    "        _, _, _, best_accuracy, _ = train_model_10(models[j], train_input, train_classes, test_input, test_target,\\\n",
    "                                                   test_classes, epochs=epochs, lr = 0.08)\n",
    "        accuracies[j][i] = best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9662, 0.9501, 0.9654, 0.9356, 0.9695, 0.9635, 0.9708, 0.9520])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0038, 0.0084, 0.0059, 0.0093, 0.0036, 0.0053, 0.0049, 0.0071])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.median(\n",
       "values=tensor([0.9660, 0.9480, 0.9650, 0.9330, 0.9680, 0.9600, 0.9690, 0.9490]),\n",
       "indices=tensor([7, 3, 1, 2, 2, 3, 0, 0]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.median(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.9710, 0.9650, 0.9720, 0.9500, 0.9770, 0.9730, 0.9810, 0.9640]),\n",
       "indices=tensor([0, 8, 8, 4, 8, 8, 7, 8]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.min(\n",
       "values=tensor([0.9580, 0.9380, 0.9550, 0.9220, 0.9660, 0.9580, 0.9650, 0.9440]),\n",
       "indices=tensor([6, 1, 6, 9, 9, 0, 3, 9]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.min(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = bigConvNet_3(500, 0.0, 0.25)\n",
    "model2 = bigConvNet_3(500, 0.25, 0.25)\n",
    "model3 = bigConvNet_3(500, 0.25, 0.5)\n",
    "\n",
    "models = [model1, model2, model3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = torch.empty(3, 10, dtype=torch.float)\n",
    "epochs = 200\n",
    "\n",
    "for i in range(10):\n",
    "    train_input, train_target, train_classes, test_input, test_target, test_classes = prolog.generate_pair_sets(1000)\n",
    "\n",
    "    train_input = train_input.view(-1, 14, 14).unsqueeze(1)\n",
    "    test_input = test_input.view(-1, 14, 14).unsqueeze(1)\n",
    "    train_classes = train_classes.view(2000)\n",
    "    test_classes = test_classes.view(2000)\n",
    "    test_target = test_target\n",
    "\n",
    "    for j in range(3):\n",
    "        _, _, _, best_accuracy, _ = train_model_10(models[j], train_input, train_classes, test_input, test_target,\\\n",
    "                                                   test_classes, epochs=epochs, lr = 0.08)\n",
    "        accuracies[j][i] = best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9723, 0.9705, 0.9672])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0049, 0.0031, 0.0036])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.median(\n",
       "values=tensor([0.9710, 0.9700, 0.9670]),\n",
       "indices=tensor([1, 9, 2]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.median(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.9790, 0.9750, 0.9730]),\n",
       "indices=tensor([8, 5, 7]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.min(\n",
       "values=tensor([0.9650, 0.9660, 0.9620]),\n",
       "indices=tensor([9, 3, 8]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.min(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=14*14, intermediate_dim=10, output_dim=2, hidden_dim=256):\n",
    "        super(Siamese, self).__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(input_dim, 1024), nn.ReLU(), nn.Linear(1024, intermediate_dim))\n",
    "        self.comparison = nn.Sequential(nn.Linear(intermediate_dim, hidden_dim), nn.ReLU(),\\\n",
    "                                        nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "    def forward1(self, x):\n",
    "        mid = self.model(x)\n",
    "        return mid\n",
    "    \n",
    "    def forward2(self, mid1, mid2):\n",
    "        mid = mid1 - mid2\n",
    "        out = self.comparison(mid)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "conv_net.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
