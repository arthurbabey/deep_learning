{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import dlc_practical_prologue as prolog\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_errors(pred, truth):\n",
    "    \n",
    "    pred_class = pred.argmax(1)\n",
    "    return (pred_class - truth != 0).sum().item()\n",
    "        \n",
    "    \n",
    "def train_model(model, train_input, train_target, test_input, test_target,  epochs=500, batch_size=100, lr=0.1):\n",
    "    \n",
    "    torch.nn.init.xavier_uniform_(model.conv1.weight)\n",
    "    torch.nn.init.xavier_uniform_(model.conv2.weight)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    #scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    best_accuracy = 0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        for b in range(0, train_input.size(0), batch_size):\n",
    "            output = model(train_input.narrow(0, b, batch_size))\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            loss = criterion(output, train_target.narrow(0, b, batch_size))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "        \n",
    "        output_train = model(train_input)\n",
    "        output_test = model(test_input)\n",
    "        train_loss.append(criterion(output_train, train_target).item())\n",
    "        test_loss.append(criterion(output_test, test_target).item())\n",
    "        accuracy = 1 - nb_errors(output_test, test_target) / 1000\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_epoch = i+1\n",
    "        test_accuracy.append(accuracy)\n",
    "        \n",
    "        if i%5 == 0:\n",
    "            print('Epoch : ',i+1, '\\t', 'test loss :', test_loss[-1], '\\t', 'train loss', train_loss[-1])\n",
    "        \n",
    "    return train_loss, test_loss, test_accuracy, best_accuracy        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet3(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(ConvNet3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 2, out_channels = 4, kernel_size=2, stride = 1)\n",
    "        self.conv2 = nn.Conv2d(4, 8, kernel_size=3, stride = 1, padding=2)\n",
    "        self.conv3 = nn.Conv2d(8, 16, kernel_size = 3, stride = 1, padding=2)\n",
    "        self.fc1 = nn.Linear(16*3*3, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        #self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = x.view(-1, 16*3*3)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = self.dropout2(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet2(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(ConvNet2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 2, out_channels = 8, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride = 1)\n",
    "        self.fc1 = nn.Linear(64, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "        #self.dropout1 = nn.Dropout2d(0.25)\n",
    "        #self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        #x = self.dropout1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 64)\n",
    "        #x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prolog.generate_pair_sets(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet2(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 \t test loss : 1.7596607208251953 \t train loss 1.5987602472305298\n",
      "Epoch :  6 \t test loss : 0.8459348082542419 \t train loss 0.6801041960716248\n",
      "Epoch :  11 \t test loss : 0.6623099446296692 \t train loss 0.47203686833381653\n",
      "Epoch :  16 \t test loss : 0.6065219640731812 \t train loss 0.37997451424598694\n",
      "Epoch :  21 \t test loss : 0.5845654010772705 \t train loss 0.3194357752799988\n",
      "Epoch :  26 \t test loss : 0.5801838636398315 \t train loss 0.2742043435573578\n",
      "Epoch :  31 \t test loss : 0.5859231352806091 \t train loss 0.22964411973953247\n",
      "Epoch :  36 \t test loss : 0.6033982634544373 \t train loss 0.19105957448482513\n",
      "Epoch :  41 \t test loss : 0.6262567043304443 \t train loss 0.15902751684188843\n",
      "Epoch :  46 \t test loss : 0.657523512840271 \t train loss 0.13236689567565918\n",
      "Epoch :  51 \t test loss : 0.6922580003738403 \t train loss 0.10970384627580643\n"
     ]
    }
   ],
   "source": [
    "_, _, _, best_accuracy = train_model(model, train_input, train_target, test_input,\\\n",
    "                                             test_target, epochs=51, lr = 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.771"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ConvNet3(200)\n",
    "model2 = ConvNet3(100)\n",
    "model3 = ConvNet3(350)\n",
    "model4 = ConvNet3(700)\n",
    "\n",
    "model5 = ConvNet2(50)\n",
    "model6 = ConvNet2(100)\n",
    "model7 = ConvNet2(350)\n",
    "model8 = ConvNet2(700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model1, model2, model3, model4, model5, model6, model7, model8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 \t test loss : 0.6930968761444092 \t train loss 0.6928606033325195\n",
      "Epoch :  11 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  21 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  31 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  41 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  51 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  61 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  71 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  81 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  91 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  101 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  111 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  121 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  131 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  141 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  151 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  161 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  171 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  181 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  191 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "0.5369999999999999\n",
      "Epoch :  1 \t test loss : 0.6931537985801697 \t train loss 0.6931422352790833\n",
      "Epoch :  11 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  21 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  31 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  41 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  51 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  61 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  71 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  81 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  91 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  101 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  111 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  121 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  131 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  141 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  151 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  161 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  171 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  181 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  191 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "0.5369999999999999\n",
      "Epoch :  1 \t test loss : 0.6933026313781738 \t train loss 0.6931537985801697\n",
      "Epoch :  11 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  21 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  31 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  41 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  51 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  61 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  71 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  81 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  91 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  101 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  111 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  121 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  131 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  141 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  151 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  161 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  171 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  181 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  191 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "0.5369999999999999\n",
      "Epoch :  1 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  11 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  21 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  31 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  41 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  51 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  61 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  71 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  81 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  91 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  101 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  111 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  121 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  131 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  141 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  151 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  161 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  171 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  181 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  191 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "0.5369999999999999\n",
      "Epoch :  1 \t test loss : 0.7085257768630981 \t train loss 0.7111907005310059\n",
      "Epoch :  11 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  21 \t test loss : 0.6935319900512695 \t train loss 0.6931537985801697\n",
      "Epoch :  31 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  41 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  51 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  61 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  71 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  81 \t test loss : 0.6941637396812439 \t train loss 0.6931537985801697\n",
      "Epoch :  91 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  101 \t test loss : 0.6931757926940918 \t train loss 0.6931537985801697\n",
      "Epoch :  111 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  121 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  131 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  141 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  151 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  161 \t test loss : 0.6962068676948547 \t train loss 0.6931537985801697\n",
      "Epoch :  171 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  181 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  191 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "0.538\n",
      "Epoch :  1 \t test loss : 0.6963286399841309 \t train loss 0.6931537985801697\n",
      "Epoch :  11 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  21 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  31 \t test loss : 0.6930741667747498 \t train loss 0.6931537985801697\n",
      "Epoch :  41 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  51 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  61 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  71 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  81 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  91 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  101 \t test loss : 0.6930237412452698 \t train loss 0.6931537985801697\n",
      "Epoch :  111 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  121 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  131 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  141 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  151 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  161 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  171 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  181 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  191 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "0.538\n",
      "Epoch :  1 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  11 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  21 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  31 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  41 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  51 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  61 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  71 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  81 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  91 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  101 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  111 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  121 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  131 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  141 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  151 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  161 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  171 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  181 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  191 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "0.5369999999999999\n",
      "Epoch :  1 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  11 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  21 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  31 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  41 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  51 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  61 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  71 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  81 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  91 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  101 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  111 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  121 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  131 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  141 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  151 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  161 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  171 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  181 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  191 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "0.5369999999999999\n",
      "Epoch :  1 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  11 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  21 \t test loss : 0.6931537985801697 \t train loss 0.6931431293487549\n",
      "Epoch :  31 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  41 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  51 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  61 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  71 \t test loss : 0.6931769847869873 \t train loss 0.6931537985801697\n",
      "Epoch :  81 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  91 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  101 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  111 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  121 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  131 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  141 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  151 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  161 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  171 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  181 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  191 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "0.5469999999999999\n",
      "Epoch :  1 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  11 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  21 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  31 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  41 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  51 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  61 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  71 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  81 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  91 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  101 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  111 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  121 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  131 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  141 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  151 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  161 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  171 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  181 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  191 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "0.546\n",
      "Epoch :  1 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  11 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  21 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  31 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  41 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  51 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  61 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  71 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  81 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  91 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  101 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  111 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  121 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  131 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  141 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  151 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  161 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  171 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  181 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  191 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "0.546\n",
      "Epoch :  1 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  11 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  21 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  31 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  41 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  51 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  61 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  71 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  81 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  91 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  101 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  111 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  121 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  131 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  141 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  151 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  161 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  171 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  181 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  191 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "0.546\n",
      "Epoch :  1 \t test loss : 0.6931537985801697 \t train loss 0.6931936740875244\n",
      "Epoch :  11 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  21 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  31 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  41 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  51 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  61 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  71 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  81 \t test loss : 0.6931537985801697 \t train loss 0.6930421590805054\n",
      "Epoch :  91 \t test loss : 0.6931537985801697 \t train loss 0.692481279373169\n",
      "Epoch :  101 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  111 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  121 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  131 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  141 \t test loss : 0.6931537985801697 \t train loss 0.694597601890564\n",
      "Epoch :  151 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  161 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  171 \t test loss : 0.6933084726333618 \t train loss 0.6931537985801697\n",
      "Epoch :  181 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  191 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "0.546\n",
      "Epoch :  1 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  11 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  21 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  31 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  41 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  51 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  61 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  71 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  81 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  91 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  101 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  111 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  121 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  131 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  141 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  151 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  161 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  171 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  181 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  191 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "0.546\n",
      "Epoch :  1 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  11 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  21 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  31 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  41 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  51 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  61 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  71 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  81 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  91 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  101 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  111 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  121 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  131 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  141 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  151 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  161 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  171 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  181 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  191 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "0.546\n",
      "Epoch :  1 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  11 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  21 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  31 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  41 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  51 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  61 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  71 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  81 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  91 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  101 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  111 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  121 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  131 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  141 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  151 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  161 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  171 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  181 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  191 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "0.546\n",
      "Epoch :  1 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  11 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  21 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  31 \t test loss : 0.6929537057876587 \t train loss 0.6931537985801697\n",
      "Epoch :  41 \t test loss : 0.6915331482887268 \t train loss 0.6973340511322021\n",
      "Epoch :  51 \t test loss : 0.6702325344085693 \t train loss 0.6733124256134033\n",
      "Epoch :  61 \t test loss : 0.6610875725746155 \t train loss 0.6573403477668762\n",
      "Epoch :  71 \t test loss : 0.658228874206543 \t train loss 0.6621551513671875\n",
      "Epoch :  81 \t test loss : 0.6536559462547302 \t train loss 0.6524945497512817\n",
      "Epoch :  91 \t test loss : 0.6553871631622314 \t train loss 0.6444341540336609\n",
      "Epoch :  101 \t test loss : 0.6548997163772583 \t train loss 0.642947256565094\n",
      "Epoch :  111 \t test loss : 0.6629484295845032 \t train loss 0.6490975022315979\n",
      "Epoch :  121 \t test loss : 0.6555048227310181 \t train loss 0.6479035019874573\n",
      "Epoch :  131 \t test loss : 0.6559574007987976 \t train loss 0.6466360092163086\n",
      "Epoch :  141 \t test loss : 0.6616489887237549 \t train loss 0.646217405796051\n",
      "Epoch :  151 \t test loss : 0.6593800783157349 \t train loss 0.6478486657142639\n",
      "Epoch :  161 \t test loss : 0.6521642208099365 \t train loss 0.6540669798851013\n",
      "Epoch :  171 \t test loss : 0.6598756313323975 \t train loss 0.6439552903175354\n",
      "Epoch :  181 \t test loss : 0.6542431116104126 \t train loss 0.6428568959236145\n",
      "Epoch :  191 \t test loss : 0.6600818037986755 \t train loss 0.6458947658538818\n",
      "0.5409999999999999\n",
      "Epoch :  1 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  11 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  21 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  31 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  41 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  51 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  61 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  71 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  81 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  91 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  101 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  111 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  121 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  131 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  141 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  151 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  161 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  171 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  181 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  191 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "0.54\n",
      "Epoch :  1 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  11 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n",
      "Epoch :  21 \t test loss : 0.6931537985801697 \t train loss 0.6931537985801697\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-071afe681ea8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         _, _, _, best_accuracy = train_model(models[j], train_input, train_target, test_input,\\\n\u001b[0;32m----> 9\u001b[0;31m                                              test_target, epochs=epochs, lr = 0.01)\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0maccuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-874784a05cde>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_input, train_target, test_input, test_target, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4e0376f84c92>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "accuracies = torch.empty(8, 10, dtype=torch.float)\n",
    "\n",
    "for i in range(10):\n",
    "    train_input, train_target, train_classes, test_input, test_target, test_classes = prolog.generate_pair_sets(1000)\n",
    "\n",
    "    for j in range(8):\n",
    "        _, _, _, best_accuracy = train_model(models[j], train_input, train_target, test_input,\\\n",
    "                                             test_target, epochs=epochs, lr = 0.01)\n",
    "        print(best_accuracy)\n",
    "        accuracies[j][i] = best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
